{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "medgan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenM1215/medgan/blob/master/medgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "kn1uEfWnzBMM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys, time, argparse\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tensorflow.contrib.layers import l2_regularizer\n",
        "from tensorflow.contrib.layers import batch_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d1GGXAwhXG_7",
        "colab_type": "code",
        "outputId": "732252f4-6d95-4d01-fabe-8a9ae80b0ea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qgTwzJtSXiSc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embed_size = 128 #The dimension size of the embedding, which will be generated by the generator. (default value: 128)\n",
        "noise_size = 128, #The dimension size of the random noise, on which the generator is conditioned. (default value: 128)\n",
        "generator_size = (128, 128) #'The dimension size of the generator. Note that another layer of size \"--embed_size\" is always added. (default value: (128, 128))\n",
        "discriminator_size = (256, 128, 1) #The dimension size of the discriminator. (default value: (256, 128, 1))\n",
        "compressor_size = () #The dimension size of the encoder of the autoencoder. Note that another layer of size \"--embed_size\" is always added. Therefore this can be a blank tuple. (default value: ())\n",
        "decompressor_size = () #The dimension size of the decoder of the autoencoder. Note that another layer, whose size is equal to the dimension of the <patient_matrix>, is always added. Therefore this can be a blank tuple. (default value: ())\n",
        "data_type = 'binary' #The input data type. The <patient matrix> could either contain binary values or count values. (default value: \"binary\")\n",
        "batchnorm_decay = 0.99 #Decay value for the moving average used in Batch Normalization. (default value: 0.99)\n",
        "L2 = 0.001 #L2 regularization coefficient for all weights. (default value: 0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zxYVUi8SXeFE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "\n",
        "if data_type == 'count':\n",
        "  base_dir = root_dir + 'GOSH/Synthetic Data/medgan/count/'\n",
        "else:\n",
        "  base_dir = root_dir + 'GOSH/Synthetic Data/medgan/binary/'\n",
        "\n",
        "raw_data_dir = root_dir + 'GOSH/Synthetic Data/medgan/mimic/'\n",
        "processed_data_dir = base_dir + 'processed_mimic/'\n",
        "model_dir = base_dir + 'models/'\n",
        "gen_data_dir = base_dir + 'generated_data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wWlnbTwyXwqT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_file = processed_data_dir + 'processed_mimic.matrix' #The path to the numpy matrix containing aggregated patient records.\n",
        "out_file = model_dir #The path to the output models.\n",
        "model_file = '' #The path to the model file, in case you want to continue training. (default value: '')\n",
        "n_pretrain_epoch = 100 #The number of epochs to pre-train the autoencoder. (default value: 100)\n",
        "n_epoch = 1000 #The number of epochs to train medGAN. (default value: 1000)\n",
        "n_discriminator_update = 2 #The number of times to update the discriminator per epoch. (default value: 2)\n",
        "n_generator_update = 1 #The number of times to update the generator per epoch. (default value: 1)\n",
        "pretrain_batch_size = 100 #The size of a single mini-batch for pre-training the autoencoder. (default value: 100)\n",
        "batch_size = 1000 #The size of a single mini-batch for training medGAN. (default value: 1000)\n",
        "save_max_keep = 1 #The number of models to keep. Setting this to 0 will save models for every epoch. (default value: 0)\n",
        "generate_data = False #If True the model generates data, if False the model is trained (default value: False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IpF7DdAbzFhq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_VALIDATION_RATIO = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "obJxPLHtzMv-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Medgan(object):\n",
        "    def __init__(self,\n",
        "                 dataType='binary',\n",
        "                 inputDim=615,\n",
        "                 embeddingDim=128,\n",
        "                 randomDim=128,\n",
        "                 generatorDims=(128, 128),\n",
        "                 discriminatorDims=(256, 128, 1),\n",
        "                 compressDims=(),\n",
        "                 decompressDims=(),\n",
        "                 bnDecay=0.99,\n",
        "                 l2scale=0.001):\n",
        "        self.inputDim = inputDim\n",
        "        self.embeddingDim = embeddingDim\n",
        "        self.generatorDims = list(generatorDims) + [embeddingDim]\n",
        "        self.randomDim = randomDim\n",
        "        self.dataType = dataType\n",
        "\n",
        "        if dataType == 'binary':\n",
        "            self.aeActivation = tf.nn.tanh\n",
        "        else:\n",
        "            self.aeActivation = tf.nn.relu\n",
        "\n",
        "        self.generatorActivation = tf.nn.relu\n",
        "        self.discriminatorActivation = tf.nn.relu\n",
        "        self.discriminatorDims = discriminatorDims\n",
        "        self.compressDims = list(compressDims) + [embeddingDim]\n",
        "        self.decompressDims = list(decompressDims) + [inputDim]\n",
        "        self.bnDecay = bnDecay\n",
        "        self.l2scale = l2scale\n",
        "\n",
        "    def loadData(self, dataPath=''):\n",
        "        data = np.load(dataPath)\n",
        "\n",
        "        if self.dataType == 'binary':\n",
        "            data = np.clip(data, 0, 1)\n",
        "\n",
        "        trainX, validX = train_test_split(data, test_size=_VALIDATION_RATIO, random_state=0)\n",
        "        return trainX, validX\n",
        "\n",
        "    def buildAutoencoder(self, x_input):\n",
        "        decodeVariables = {}\n",
        "        with tf.variable_scope('autoencoder', regularizer=l2_regularizer(self.l2scale)):\n",
        "            tempVec = x_input\n",
        "            tempDim = self.inputDim\n",
        "            i = 0\n",
        "            for compressDim in self.compressDims:\n",
        "                W = tf.get_variable('aee_W_'+str(i), shape=[tempDim, compressDim])\n",
        "                b = tf.get_variable('aee_b_'+str(i), shape=[compressDim])\n",
        "                tempVec = self.aeActivation(tf.add(tf.matmul(tempVec, W), b))\n",
        "                tempDim = compressDim\n",
        "                i += 1\n",
        "    \n",
        "            i = 0\n",
        "            for decompressDim in self.decompressDims[:-1]:\n",
        "                W = tf.get_variable('aed_W_'+str(i), shape=[tempDim, decompressDim])\n",
        "                b = tf.get_variable('aed_b_'+str(i), shape=[decompressDim])\n",
        "                tempVec = self.aeActivation(tf.add(tf.matmul(tempVec, W), b))\n",
        "                tempDim = decompressDim\n",
        "                decodeVariables['aed_W_'+str(i)] = W\n",
        "                decodeVariables['aed_b_'+str(i)] = b\n",
        "                i += 1\n",
        "            W = tf.get_variable('aed_W_'+str(i), shape=[tempDim, self.decompressDims[-1]])\n",
        "            b = tf.get_variable('aed_b_'+str(i), shape=[self.decompressDims[-1]])\n",
        "            decodeVariables['aed_W_'+str(i)] = W\n",
        "            decodeVariables['aed_b_'+str(i)] = b\n",
        "\n",
        "            if self.dataType == 'binary':\n",
        "                x_reconst = tf.nn.sigmoid(tf.add(tf.matmul(tempVec,W),b))\n",
        "                loss = tf.reduce_mean(-tf.reduce_sum(x_input * tf.log(x_reconst + 1e-12) + (1. - x_input) * tf.log(1. - x_reconst + 1e-12), 1), 0)\n",
        "            else:\n",
        "                x_reconst = tf.nn.relu(tf.add(tf.matmul(tempVec,W),b))\n",
        "                loss = tf.reduce_mean((x_input - x_reconst)**2)\n",
        "            \n",
        "        return loss, decodeVariables\n",
        "\n",
        "    def buildGenerator(self, x_input, bn_train):\n",
        "        tempVec = x_input\n",
        "        tempDim = int(self.randomDim[0])\n",
        "        with tf.variable_scope('generator', regularizer=l2_regularizer(self.l2scale)):\n",
        "            for i, genDim in enumerate(self.generatorDims[:-1]):\n",
        "                #print(tempDim, genDim)\n",
        "                W = tf.get_variable('W_'+str(i), shape=[tempDim, genDim])\n",
        "                h = tf.matmul(tempVec,W)\n",
        "                h2 = batch_norm(h, decay=self.bnDecay, scale=True, is_training=bn_train, updates_collections=None)\n",
        "                h3 = self.generatorActivation(h2)\n",
        "                tempVec = h3 + tempVec\n",
        "                tempDim = genDim\n",
        "            W = tf.get_variable('W'+str(i), shape=[tempDim, self.generatorDims[-1]])\n",
        "            h = tf.matmul(tempVec,W)\n",
        "            h2 = batch_norm(h, decay=self.bnDecay, scale=True, is_training=bn_train, updates_collections=None)\n",
        "\n",
        "            if self.dataType == 'binary':\n",
        "                h3 = tf.nn.tanh(h2)\n",
        "            else:\n",
        "                h3 = tf.nn.relu(h2)\n",
        "\n",
        "            output = h3 + tempVec\n",
        "        return output\n",
        "    \n",
        "    def buildGeneratorTest(self, x_input, bn_train):\n",
        "        tempVec = x_input\n",
        "        tempDim = int(self.randomDim[0])\n",
        "        with tf.variable_scope('generator', regularizer=l2_regularizer(self.l2scale)):\n",
        "            for i, genDim in enumerate(self.generatorDims[:-1]):\n",
        "                W = tf.get_variable('W_'+str(i), shape=[tempDim, genDim])\n",
        "                h = tf.matmul(tempVec,W)\n",
        "                h2 = batch_norm(h, decay=self.bnDecay, scale=True, is_training=bn_train, updates_collections=None, trainable=False)\n",
        "                h3 = self.generatorActivation(h2)\n",
        "                tempVec = h3 + tempVec\n",
        "                tempDim = genDim\n",
        "            W = tf.get_variable('W'+str(i), shape=[tempDim, self.generatorDims[-1]])\n",
        "            h = tf.matmul(tempVec,W)\n",
        "            h2 = batch_norm(h, decay=self.bnDecay, scale=True, is_training=bn_train, updates_collections=None, trainable=False)\n",
        "\n",
        "            if self.dataType == 'binary':\n",
        "                h3 = tf.nn.tanh(h2)\n",
        "            else:\n",
        "                h3 = tf.nn.relu(h2)\n",
        "\n",
        "            output = h3 + tempVec\n",
        "        return output\n",
        "    \n",
        "    def getDiscriminatorResults(self, x_input, keepRate, reuse=False):\n",
        "        batchSize = tf.shape(x_input)[0]\n",
        "        inputMean = tf.reshape(tf.tile(tf.reduce_mean(x_input,0), [batchSize]), (batchSize, self.inputDim))\n",
        "        tempVec = tf.concat([x_input, inputMean], 1)\n",
        "        tempDim = self.inputDim * 2\n",
        "        with tf.variable_scope('discriminator', reuse=reuse, regularizer=l2_regularizer(self.l2scale)):\n",
        "            for i, discDim in enumerate(self.discriminatorDims[:-1]):\n",
        "                W = tf.get_variable('W_'+str(i), shape=[tempDim, discDim])\n",
        "                b = tf.get_variable('b_'+str(i), shape=[discDim])\n",
        "                h = self.discriminatorActivation(tf.add(tf.matmul(tempVec,W),b))\n",
        "                h = tf.nn.dropout(h, keepRate)\n",
        "                tempVec = h\n",
        "                tempDim = discDim\n",
        "            W = tf.get_variable('W', shape=[tempDim, 1])\n",
        "            b = tf.get_variable('b', shape=[1])\n",
        "            y_hat = tf.squeeze(tf.nn.sigmoid(tf.add(tf.matmul(tempVec, W), b)))\n",
        "        return y_hat\n",
        "    \n",
        "    def buildDiscriminator(self, x_real, x_fake, keepRate, decodeVariables, bn_train):\n",
        "        #Discriminate for real samples\n",
        "        y_hat_real = self.getDiscriminatorResults(x_real, keepRate, reuse=False)\n",
        "\n",
        "        #Decompress, then discriminate for real samples\n",
        "        tempVec = x_fake\n",
        "        i = 0\n",
        "        for _ in self.decompressDims[:-1]:\n",
        "            tempVec = self.aeActivation(tf.add(tf.matmul(tempVec, decodeVariables['aed_W_'+str(i)]), decodeVariables['aed_b_'+str(i)]))\n",
        "            i += 1\n",
        "\n",
        "        if self.dataType == 'binary':\n",
        "            x_decoded = tf.nn.sigmoid(tf.add(tf.matmul(tempVec, decodeVariables['aed_W_'+str(i)]), decodeVariables['aed_b_'+str(i)]))\n",
        "        else:\n",
        "            x_decoded = tf.nn.relu(tf.add(tf.matmul(tempVec, decodeVariables['aed_W_'+str(i)]), decodeVariables['aed_b_'+str(i)]))\n",
        "\n",
        "        y_hat_fake = self.getDiscriminatorResults(x_decoded, keepRate, reuse=True)\n",
        "\n",
        "        loss_d = -tf.reduce_mean(tf.log(y_hat_real + 1e-12)) - tf.reduce_mean(tf.log(1. - y_hat_fake + 1e-12))\n",
        "        loss_g = -tf.reduce_mean(tf.log(y_hat_fake + 1e-12))\n",
        "\n",
        "        return loss_d, loss_g, y_hat_real, y_hat_fake\n",
        "\n",
        "    def print2file(self, buf, outFile):\n",
        "        outfd = open(outFile, 'a')\n",
        "        outfd.write(buf + '\\n')\n",
        "        outfd.close()\n",
        "    \n",
        "    def generateData(self,\n",
        "                     nSamples=100,\n",
        "                     modelFile='model',\n",
        "                     batchSize=100,\n",
        "                     outFile='out'):\n",
        "        x_dummy = tf.placeholder('float', [None, self.inputDim])\n",
        "        _, decodeVariables = self.buildAutoencoder(x_dummy)\n",
        "        x_random = tf.placeholder('float', [None, int(self.randomDim[0])])\n",
        "        bn_train = tf.placeholder('bool')\n",
        "        x_emb = self.buildGeneratorTest(x_random, bn_train)\n",
        "        tempVec = x_emb\n",
        "        i = 0\n",
        "        for _ in self.decompressDims[:-1]:\n",
        "            tempVec = self.aeActivation(tf.add(tf.matmul(tempVec, decodeVariables['aed_W_'+str(i)]), decodeVariables['aed_b_'+str(i)]))\n",
        "            i += 1\n",
        "\n",
        "        if self.dataType == 'binary':\n",
        "            x_reconst = tf.nn.sigmoid(tf.add(tf.matmul(tempVec, decodeVariables['aed_W_'+str(i)]), decodeVariables['aed_b_'+str(i)]))\n",
        "        else:\n",
        "            x_reconst = tf.nn.relu(tf.add(tf.matmul(tempVec, decodeVariables['aed_W_'+str(i)]), decodeVariables['aed_b_'+str(i)]))\n",
        "\n",
        "        np.random.seed(1234)\n",
        "        saver = tf.train.Saver()\n",
        "        outputVec = []\n",
        "        burn_in = 1000\n",
        "        with tf.Session() as sess:\n",
        "            saver.restore(sess, modelFile)\n",
        "            print('burning in')\n",
        "            for i in range(burn_in):\n",
        "                randomX = np.random.normal(size=(batchSize, int(self.randomDim[0])))\n",
        "                output = sess.run(x_reconst, feed_dict={x_random:randomX, bn_train:True})\n",
        "\n",
        "            print('generating')\n",
        "            nBatches = int(np.ceil(float(nSamples)) / float(batchSize))\n",
        "            for i in range(nBatches):\n",
        "                randomX = np.random.normal(size=(batchSize, int(self.randomDim[0])))\n",
        "                output = sess.run(x_reconst, feed_dict={x_random:randomX, bn_train:False})\n",
        "                outputVec.extend(output)\n",
        "\n",
        "        outputMat = np.array(outputVec)\n",
        "        np.save(outFile, outputMat)\n",
        "    \n",
        "    def calculateDiscAuc(self, preds_real, preds_fake):\n",
        "        preds = np.concatenate([preds_real, preds_fake], axis=0)\n",
        "        labels = np.concatenate([np.ones((len(preds_real))), np.zeros((len(preds_fake)))], axis=0)\n",
        "        auc = roc_auc_score(labels, preds)\n",
        "        return auc\n",
        "    \n",
        "    def calculateDiscAccuracy(self, preds_real, preds_fake):\n",
        "        total = len(preds_real) + len(preds_fake)\n",
        "        hit = 0\n",
        "        for pred in preds_real: \n",
        "            if pred > 0.5: hit += 1\n",
        "        for pred in preds_fake: \n",
        "            if pred < 0.5: hit += 1\n",
        "        acc = float(hit) / float(total)\n",
        "        return acc\n",
        "\n",
        "    def train(self,\n",
        "              dataPath='data',\n",
        "              modelPath='',\n",
        "              outPath='out',\n",
        "              nEpochs=500,\n",
        "              discriminatorTrainPeriod=2,\n",
        "              generatorTrainPeriod=1,\n",
        "              pretrainBatchSize=100,\n",
        "              batchSize=1000,\n",
        "              pretrainEpochs=100,\n",
        "              saveMaxKeep=0):\n",
        "        x_raw = tf.placeholder('float', [None, self.inputDim])\n",
        "        x_random = tf.placeholder('float', [None, int(self.randomDim[0])])\n",
        "        keep_prob = tf.placeholder('float')\n",
        "        bn_train = tf.placeholder('bool')\n",
        "\n",
        "        loss_ae, decodeVariables = self.buildAutoencoder(x_raw)\n",
        "        x_fake = self.buildGenerator(x_random, bn_train)\n",
        "        loss_d, loss_g, y_hat_real, y_hat_fake = self.buildDiscriminator(x_raw, x_fake, keep_prob, decodeVariables, bn_train)\n",
        "        trainX, validX = self.loadData(dataPath)\n",
        "\n",
        "        t_vars = tf.trainable_variables()\n",
        "        ae_vars = [var for var in t_vars if 'autoencoder' in var.name]\n",
        "        d_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
        "        g_vars = [var for var in t_vars if 'generator' in var.name]\n",
        "\n",
        "        all_regs = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "\n",
        "        optimize_ae = tf.train.AdamOptimizer().minimize(loss_ae + sum(all_regs), var_list=ae_vars)\n",
        "        optimize_d = tf.train.AdamOptimizer().minimize(loss_d + sum(all_regs), var_list=d_vars)\n",
        "        decodeVariablesValues = list(decodeVariables.values())\n",
        "        optimize_g = tf.train.AdamOptimizer().minimize(loss_g + sum(all_regs), var_list=g_vars+decodeVariablesValues)\n",
        "\n",
        "        initOp = tf.global_variables_initializer()\n",
        "\n",
        "        nBatches = int(np.ceil(float(trainX.shape[0]) / float(batchSize)))\n",
        "        saver = tf.train.Saver(max_to_keep=saveMaxKeep)\n",
        "        logFile = outPath + '.log'\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            if modelPath == '': sess.run(initOp)\n",
        "            else: saver.restore(sess, modelPath)\n",
        "            nTrainBatches = int(np.ceil(float(trainX.shape[0])) / float(pretrainBatchSize))\n",
        "            nValidBatches = int(np.ceil(float(validX.shape[0])) / float(pretrainBatchSize))\n",
        "\n",
        "            if modelPath== '':\n",
        "                for epoch in range(pretrainEpochs):\n",
        "                    idx = np.random.permutation(trainX.shape[0])\n",
        "                    trainLossVec = []\n",
        "                    for i in range(nTrainBatches):\n",
        "                        batchX = trainX[idx[i*pretrainBatchSize:(i+1)*pretrainBatchSize]]\n",
        "                        _, loss = sess.run([optimize_ae, loss_ae], feed_dict={x_raw:batchX})\n",
        "                        trainLossVec.append(loss)\n",
        "                    idx = np.random.permutation(validX.shape[0])\n",
        "                    validLossVec = []\n",
        "                    for i in range(nValidBatches):\n",
        "                        batchX = validX[idx[i*pretrainBatchSize:(i+1)*pretrainBatchSize]]\n",
        "                        loss = sess.run(loss_ae, feed_dict={x_raw:batchX})\n",
        "                        validLossVec.append(loss)\n",
        "                    validReverseLoss = 0.\n",
        "                    buf = 'Pretrain_Epoch:%d, trainLoss:%f, validLoss:%f, validReverseLoss:%f' % (epoch, np.mean(trainLossVec), np.mean(validLossVec), validReverseLoss)\n",
        "                    print(buf)\n",
        "                    self.print2file(buf, logFile)\n",
        "\n",
        "            idx = np.arange(trainX.shape[0])\n",
        "            for epoch in range(nEpochs):\n",
        "                d_loss_vec= []\n",
        "                g_loss_vec = []\n",
        "                for i in range(nBatches):\n",
        "                    for _ in range(discriminatorTrainPeriod):\n",
        "                        batchIdx = np.random.choice(idx, size=batchSize, replace=False)\n",
        "                        batchX = trainX[batchIdx]\n",
        "                        randomX = np.random.normal(size=(batchSize, int(self.randomDim[0])))\n",
        "                        _, discLoss = sess.run([optimize_d, loss_d], feed_dict={x_raw:batchX, x_random:randomX, keep_prob:1.0, bn_train:False})\n",
        "                        d_loss_vec.append(discLoss)\n",
        "                    for _ in range(generatorTrainPeriod):\n",
        "                        randomX = np.random.normal(size=(batchSize, int(self.randomDim[0])))\n",
        "                        _, generatorLoss = sess.run([optimize_g, loss_g], feed_dict={x_raw:batchX, x_random:randomX, keep_prob:1.0, bn_train:True})\n",
        "                        g_loss_vec.append(generatorLoss)\n",
        "\n",
        "                idx = np.arange(len(validX))\n",
        "                nValidBatches = int(np.ceil(float(len(validX)) / float(batchSize)))\n",
        "                validAccVec = []\n",
        "                validAucVec = []\n",
        "                for i in range(nBatches):\n",
        "                    batchIdx = np.random.choice(idx, size=batchSize, replace=False)\n",
        "                    batchX = validX[batchIdx]\n",
        "                    randomX = np.random.normal(size=(batchSize, int(self.randomDim[0])))\n",
        "                    preds_real, preds_fake, = sess.run([y_hat_real, y_hat_fake], feed_dict={x_raw:batchX, x_random:randomX, keep_prob:1.0, bn_train:False})\n",
        "                    validAcc = self.calculateDiscAccuracy(preds_real, preds_fake)\n",
        "                    validAuc = self.calculateDiscAuc(preds_real, preds_fake)\n",
        "                    validAccVec.append(validAcc)\n",
        "                    validAucVec.append(validAuc)\n",
        "                buf = 'Epoch:%d, d_loss:%f, g_loss:%f, accuracy:%f, AUC:%f' % (epoch, np.mean(d_loss_vec), np.mean(g_loss_vec), np.mean(validAccVec), np.mean(validAucVec))\n",
        "                print(buf)\n",
        "                self.print2file(buf, logFile)\n",
        "                savePath = saver.save(sess, outPath, global_step=epoch)\n",
        "        print(savePath)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4FAToZpkzSco",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def str2bool(v):\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Boolean value expected.')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HccpHKqAzbba",
        "colab_type": "code",
        "outputId": "e476472f-221c-44d1-a798-b85aa3e9d2ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18805
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "data = np.load(data_file)\n",
        "inputDim = data.shape[1]\n",
        "\n",
        "mg = Medgan(dataType = data_type,\n",
        "            inputDim = inputDim,\n",
        "            embeddingDim = embed_size,\n",
        "            randomDim = noise_size,\n",
        "            generatorDims = generator_size,\n",
        "            discriminatorDims = discriminator_size,\n",
        "            compressDims = compressor_size,\n",
        "            decompressDims = decompressor_size,\n",
        "            bnDecay = batchnorm_decay,\n",
        "            l2scale = L2)\n",
        "\n",
        "# True for generation, False for training\n",
        "if not generate_data:\n",
        "# Training\n",
        "    mg.train(dataPath = data_file,\n",
        "             modelPath = model_file,\n",
        "             outPath = out_file,\n",
        "             pretrainEpochs = n_pretrain_epoch,\n",
        "             nEpochs = n_epoch,\n",
        "             discriminatorTrainPeriod = n_discriminator_update,\n",
        "             generatorTrainPeriod = n_generator_update,\n",
        "             pretrainBatchSize = pretrain_batch_size,\n",
        "             batchSize = batch_size,\n",
        "             saveMaxKeep = save_max_keep)\n",
        "else:\n",
        "# Generate synthetic data using a trained model\n",
        "# You must specify \"--model_file\" and \"<out_file>\" to generate synthetic data.\n",
        "    mg.generateData(nSamples = 10000,\n",
        "                    modelFile = model_file,\n",
        "                    batchSize = batch_size,\n",
        "                    outFile = out_file)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pretrain_Epoch:0, trainLoss:421.710144, validLoss:86.942276, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:1, trainLoss:80.944550, validLoss:77.178452, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:2, trainLoss:74.881447, validLoss:72.689583, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:3, trainLoss:70.552689, validLoss:67.765701, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:4, trainLoss:66.213562, validLoss:63.527111, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:5, trainLoss:61.952061, validLoss:59.267391, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:6, trainLoss:58.034332, validLoss:55.798805, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:7, trainLoss:55.236309, validLoss:53.468071, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:8, trainLoss:52.746826, validLoss:50.936188, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:9, trainLoss:50.200024, validLoss:48.456459, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:10, trainLoss:47.564499, validLoss:45.810101, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:11, trainLoss:44.904438, validLoss:43.314999, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:12, trainLoss:42.343472, validLoss:41.070629, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:13, trainLoss:40.040699, validLoss:39.082729, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:14, trainLoss:38.041626, validLoss:37.483349, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:15, trainLoss:36.381626, validLoss:36.061916, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:16, trainLoss:35.010098, validLoss:35.015915, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:17, trainLoss:33.875504, validLoss:33.991642, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:18, trainLoss:32.904690, validLoss:33.273964, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:19, trainLoss:32.105011, validLoss:32.686596, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:20, trainLoss:31.393288, validLoss:32.053986, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:21, trainLoss:30.777727, validLoss:31.578249, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:22, trainLoss:30.262520, validLoss:31.329027, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:23, trainLoss:29.817940, validLoss:30.995028, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:24, trainLoss:29.446568, validLoss:30.652447, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:25, trainLoss:29.166245, validLoss:30.570276, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:26, trainLoss:28.913572, validLoss:30.331989, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:27, trainLoss:28.701656, validLoss:30.265545, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:28, trainLoss:28.517326, validLoss:29.955511, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:29, trainLoss:28.362791, validLoss:29.992090, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:30, trainLoss:28.193262, validLoss:29.751062, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:31, trainLoss:28.045692, validLoss:29.602123, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:32, trainLoss:27.913275, validLoss:29.654718, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:33, trainLoss:27.782602, validLoss:29.568893, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:34, trainLoss:27.635340, validLoss:29.434631, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:35, trainLoss:27.538208, validLoss:29.256947, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:36, trainLoss:27.443382, validLoss:29.208410, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:37, trainLoss:27.341537, validLoss:29.122253, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:38, trainLoss:27.246973, validLoss:29.045696, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:39, trainLoss:27.159304, validLoss:29.113043, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:40, trainLoss:27.079685, validLoss:28.923660, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:41, trainLoss:27.021597, validLoss:28.951208, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:42, trainLoss:26.961222, validLoss:28.915791, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:43, trainLoss:26.930218, validLoss:28.868923, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:44, trainLoss:26.881645, validLoss:28.941479, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:45, trainLoss:26.849005, validLoss:28.841969, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:46, trainLoss:26.811422, validLoss:28.880678, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:47, trainLoss:26.783268, validLoss:28.817781, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:48, trainLoss:26.745878, validLoss:28.771870, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:49, trainLoss:26.694315, validLoss:28.824415, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:50, trainLoss:26.666931, validLoss:28.629318, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:51, trainLoss:26.609236, validLoss:28.684565, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:52, trainLoss:26.550362, validLoss:28.625521, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:53, trainLoss:26.475597, validLoss:28.539009, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:54, trainLoss:26.431192, validLoss:28.489902, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:55, trainLoss:26.365341, validLoss:28.471231, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:56, trainLoss:26.310211, validLoss:28.461882, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:57, trainLoss:26.259195, validLoss:28.393751, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:58, trainLoss:26.229479, validLoss:28.319555, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:59, trainLoss:26.199612, validLoss:28.394003, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:60, trainLoss:26.177055, validLoss:28.248203, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:61, trainLoss:26.142744, validLoss:28.333876, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:62, trainLoss:26.119892, validLoss:28.238073, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:63, trainLoss:26.108036, validLoss:28.253386, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:64, trainLoss:26.079214, validLoss:28.249815, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:65, trainLoss:26.066891, validLoss:28.307068, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:66, trainLoss:26.047018, validLoss:28.274704, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:67, trainLoss:26.025530, validLoss:28.268370, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:68, trainLoss:26.024519, validLoss:28.242712, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:69, trainLoss:25.997614, validLoss:28.199133, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:70, trainLoss:25.983221, validLoss:28.278833, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:71, trainLoss:25.974010, validLoss:28.106905, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:72, trainLoss:25.970530, validLoss:28.117460, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:73, trainLoss:25.963890, validLoss:28.202921, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:74, trainLoss:25.948450, validLoss:28.168329, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:75, trainLoss:25.940420, validLoss:28.217937, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:76, trainLoss:25.931545, validLoss:28.127514, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:77, trainLoss:25.929052, validLoss:28.131502, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:78, trainLoss:25.900101, validLoss:28.227959, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:79, trainLoss:25.909264, validLoss:28.037966, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:80, trainLoss:25.909876, validLoss:28.127468, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:81, trainLoss:25.894569, validLoss:28.191399, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:82, trainLoss:25.888811, validLoss:28.217510, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:83, trainLoss:25.883314, validLoss:28.178858, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:84, trainLoss:25.871048, validLoss:28.166964, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:85, trainLoss:25.869463, validLoss:28.144016, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:86, trainLoss:25.866991, validLoss:28.145351, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:87, trainLoss:25.862686, validLoss:28.067822, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:88, trainLoss:25.849726, validLoss:28.065018, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:89, trainLoss:25.849539, validLoss:28.110617, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:90, trainLoss:25.841913, validLoss:28.093225, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:91, trainLoss:25.825615, validLoss:28.112257, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:92, trainLoss:25.826872, validLoss:28.154255, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:93, trainLoss:25.825098, validLoss:28.144995, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:94, trainLoss:25.814117, validLoss:28.055262, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:95, trainLoss:25.823828, validLoss:28.085661, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:96, trainLoss:25.813990, validLoss:28.091499, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:97, trainLoss:25.806229, validLoss:28.129848, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:98, trainLoss:25.798162, validLoss:28.104912, validReverseLoss:0.000000\n",
            "Pretrain_Epoch:99, trainLoss:25.786633, validLoss:28.021734, validReverseLoss:0.000000\n",
            "Epoch:0, d_loss:0.081674, g_loss:27.539429, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:1, d_loss:0.001457, g_loss:27.631021, accuracy:1.000000, AUC:1.000000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "Epoch:2, d_loss:0.001025, g_loss:27.631021, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:3, d_loss:0.000950, g_loss:25.268778, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:4, d_loss:0.000840, g_loss:0.000497, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:5, d_loss:0.000712, g_loss:0.000612, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:6, d_loss:0.000620, g_loss:0.001175, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:7, d_loss:0.000567, g_loss:0.004962, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:8, d_loss:0.000602, g_loss:0.017439, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:9, d_loss:0.001040, g_loss:0.372102, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:10, d_loss:0.001088, g_loss:3.124883, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:11, d_loss:0.001076, g_loss:3.055151, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:12, d_loss:0.002053, g_loss:0.908941, accuracy:0.999833, AUC:1.000000\n",
            "Epoch:13, d_loss:0.005872, g_loss:4.903954, accuracy:0.999976, AUC:1.000000\n",
            "Epoch:14, d_loss:0.004717, g_loss:5.136587, accuracy:0.999940, AUC:1.000000\n",
            "Epoch:15, d_loss:0.003061, g_loss:2.447266, accuracy:0.999690, AUC:1.000000\n",
            "Epoch:16, d_loss:0.002081, g_loss:7.870547, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:17, d_loss:0.001520, g_loss:6.394527, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:18, d_loss:0.001625, g_loss:5.978280, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:19, d_loss:0.001711, g_loss:7.119180, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:20, d_loss:0.001404, g_loss:7.556534, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:21, d_loss:0.001698, g_loss:7.660236, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:22, d_loss:0.002012, g_loss:5.874958, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:23, d_loss:0.001732, g_loss:6.482399, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:24, d_loss:0.001515, g_loss:7.874298, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:25, d_loss:0.001593, g_loss:7.393753, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:26, d_loss:0.001493, g_loss:3.242891, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:27, d_loss:0.002040, g_loss:2.257647, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:28, d_loss:0.002148, g_loss:6.415827, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:29, d_loss:0.001348, g_loss:7.359552, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:30, d_loss:0.001304, g_loss:7.405735, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:31, d_loss:0.001555, g_loss:7.698209, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:32, d_loss:0.005056, g_loss:9.900320, accuracy:0.998940, AUC:1.000000\n",
            "Epoch:33, d_loss:0.011029, g_loss:25.271921, accuracy:0.998798, AUC:1.000000\n",
            "Epoch:34, d_loss:0.000600, g_loss:27.631021, accuracy:0.999869, AUC:1.000000\n",
            "Epoch:35, d_loss:0.000478, g_loss:26.809286, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:36, d_loss:0.000408, g_loss:22.988600, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:37, d_loss:0.000422, g_loss:18.079666, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:38, d_loss:0.000447, g_loss:9.341957, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:39, d_loss:0.000552, g_loss:1.058355, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:40, d_loss:0.000573, g_loss:2.842343, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:41, d_loss:0.000656, g_loss:3.448970, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:42, d_loss:0.000769, g_loss:6.450156, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:43, d_loss:0.000807, g_loss:1.680957, accuracy:0.999988, AUC:1.000000\n",
            "Epoch:44, d_loss:0.001079, g_loss:2.544612, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:45, d_loss:0.001411, g_loss:3.444747, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:46, d_loss:0.011720, g_loss:3.495855, accuracy:0.995238, AUC:0.999136\n",
            "Epoch:47, d_loss:0.002444, g_loss:14.787188, accuracy:0.999762, AUC:1.000000\n",
            "Epoch:48, d_loss:0.002515, g_loss:10.763750, accuracy:0.999881, AUC:1.000000\n",
            "Epoch:49, d_loss:0.002216, g_loss:9.276871, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:50, d_loss:0.001326, g_loss:11.271638, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:51, d_loss:0.001157, g_loss:8.325904, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:52, d_loss:0.002160, g_loss:5.984318, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:53, d_loss:0.004697, g_loss:8.380842, accuracy:0.999750, AUC:0.999982\n",
            "Epoch:54, d_loss:0.003990, g_loss:4.336473, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:55, d_loss:0.001687, g_loss:9.152968, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:56, d_loss:0.001561, g_loss:9.591471, accuracy:0.999988, AUC:1.000000\n",
            "Epoch:57, d_loss:0.002687, g_loss:5.356358, accuracy:0.999988, AUC:1.000000\n",
            "Epoch:58, d_loss:0.002061, g_loss:7.280024, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:59, d_loss:0.001491, g_loss:7.978338, accuracy:0.999964, AUC:0.999999\n",
            "Epoch:60, d_loss:0.002842, g_loss:7.339166, accuracy:0.999667, AUC:1.000000\n",
            "Epoch:61, d_loss:0.046709, g_loss:17.311646, accuracy:0.999548, AUC:0.999999\n",
            "Epoch:62, d_loss:0.001437, g_loss:26.225903, accuracy:0.995024, AUC:0.999776\n",
            "Epoch:63, d_loss:0.070721, g_loss:14.344698, accuracy:0.998976, AUC:1.000000\n",
            "Epoch:64, d_loss:0.000657, g_loss:25.873867, accuracy:0.999226, AUC:1.000000\n",
            "Epoch:65, d_loss:0.000638, g_loss:13.606692, accuracy:0.999988, AUC:1.000000\n",
            "Epoch:66, d_loss:0.000361, g_loss:16.092825, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:67, d_loss:0.000343, g_loss:20.814419, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:68, d_loss:0.000352, g_loss:11.816883, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:69, d_loss:0.000550, g_loss:10.549581, accuracy:0.999893, AUC:1.000000\n",
            "Epoch:70, d_loss:0.000527, g_loss:8.889009, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:71, d_loss:0.000427, g_loss:9.557261, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:72, d_loss:0.000419, g_loss:8.939136, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:73, d_loss:0.000439, g_loss:8.300908, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:74, d_loss:0.000469, g_loss:8.831648, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:75, d_loss:0.000513, g_loss:8.389174, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:76, d_loss:0.000545, g_loss:8.527679, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:77, d_loss:0.000591, g_loss:8.017506, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:78, d_loss:0.000707, g_loss:6.305102, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:79, d_loss:0.000987, g_loss:3.777786, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:80, d_loss:0.001890, g_loss:2.763141, accuracy:0.999893, AUC:1.000000\n",
            "Epoch:81, d_loss:0.007010, g_loss:2.712043, accuracy:0.999595, AUC:0.999990\n",
            "Epoch:82, d_loss:0.003620, g_loss:7.788930, accuracy:0.999810, AUC:1.000000\n",
            "Epoch:83, d_loss:0.011350, g_loss:4.979835, accuracy:0.999702, AUC:1.000000\n",
            "Epoch:84, d_loss:0.002480, g_loss:7.911251, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:85, d_loss:0.001019, g_loss:8.866218, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:86, d_loss:0.001026, g_loss:8.184513, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:87, d_loss:0.001305, g_loss:7.765572, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:88, d_loss:0.002092, g_loss:7.619550, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:89, d_loss:0.002756, g_loss:7.599351, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:90, d_loss:0.005556, g_loss:10.667423, accuracy:0.999369, AUC:1.000000\n",
            "Epoch:91, d_loss:0.005596, g_loss:8.574636, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:92, d_loss:0.002597, g_loss:10.229504, accuracy:0.999738, AUC:1.000000\n",
            "Epoch:93, d_loss:0.002944, g_loss:8.228209, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:94, d_loss:0.001638, g_loss:7.458829, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:95, d_loss:0.002766, g_loss:6.984368, accuracy:0.999952, AUC:1.000000\n",
            "Epoch:96, d_loss:0.005287, g_loss:8.006300, accuracy:0.999798, AUC:1.000000\n",
            "Epoch:97, d_loss:0.002485, g_loss:7.455744, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:98, d_loss:0.001518, g_loss:7.806545, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:99, d_loss:0.001977, g_loss:8.121771, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:100, d_loss:0.009552, g_loss:11.152478, accuracy:0.762024, AUC:0.989812\n",
            "Epoch:101, d_loss:0.078418, g_loss:17.185516, accuracy:0.995250, AUC:0.999058\n",
            "Epoch:102, d_loss:0.005386, g_loss:11.490037, accuracy:0.996250, AUC:0.999241\n",
            "Epoch:103, d_loss:0.002412, g_loss:9.060167, accuracy:0.997810, AUC:0.999765\n",
            "Epoch:104, d_loss:0.002138, g_loss:9.244134, accuracy:0.999345, AUC:1.000000\n",
            "Epoch:105, d_loss:0.001063, g_loss:10.297401, accuracy:0.999821, AUC:1.000000\n",
            "Epoch:106, d_loss:0.001519, g_loss:8.263144, accuracy:0.999964, AUC:1.000000\n",
            "Epoch:107, d_loss:0.001439, g_loss:8.184740, accuracy:0.999857, AUC:1.000000\n",
            "Epoch:108, d_loss:0.002725, g_loss:8.857135, accuracy:0.999619, AUC:1.000000\n",
            "Epoch:109, d_loss:0.006353, g_loss:10.999173, accuracy:0.999321, AUC:1.000000\n",
            "Epoch:110, d_loss:0.009819, g_loss:14.235154, accuracy:0.998619, AUC:0.999996\n",
            "Epoch:111, d_loss:0.002414, g_loss:8.903725, accuracy:0.999845, AUC:1.000000\n",
            "Epoch:112, d_loss:0.001968, g_loss:8.609845, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:113, d_loss:0.001672, g_loss:7.448685, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:114, d_loss:0.001845, g_loss:7.367531, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:115, d_loss:0.002494, g_loss:7.081350, accuracy:0.999988, AUC:1.000000\n",
            "Epoch:116, d_loss:0.003127, g_loss:8.821253, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:117, d_loss:0.012209, g_loss:10.605904, accuracy:0.998298, AUC:0.999996\n",
            "Epoch:118, d_loss:0.019551, g_loss:9.976383, accuracy:0.987345, AUC:0.998842\n",
            "Epoch:119, d_loss:0.011165, g_loss:7.925126, accuracy:0.997643, AUC:0.999991\n",
            "Epoch:120, d_loss:0.001947, g_loss:7.328057, accuracy:0.999726, AUC:1.000000\n",
            "Epoch:121, d_loss:0.000957, g_loss:8.683754, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:122, d_loss:0.000807, g_loss:8.936320, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:123, d_loss:0.001158, g_loss:7.794636, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:124, d_loss:0.001933, g_loss:7.433702, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:125, d_loss:0.002224, g_loss:8.002770, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:126, d_loss:0.003796, g_loss:7.474207, accuracy:0.999905, AUC:1.000000\n",
            "Epoch:127, d_loss:0.009050, g_loss:8.568549, accuracy:0.998583, AUC:0.999997\n",
            "Epoch:128, d_loss:0.004951, g_loss:10.383395, accuracy:0.999893, AUC:1.000000\n",
            "Epoch:129, d_loss:0.002629, g_loss:4.537449, accuracy:0.999345, AUC:1.000000\n",
            "Epoch:130, d_loss:0.004309, g_loss:5.337187, accuracy:0.999571, AUC:1.000000\n",
            "Epoch:131, d_loss:0.002975, g_loss:6.549945, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:132, d_loss:0.003016, g_loss:6.646035, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:133, d_loss:0.004235, g_loss:1.390288, accuracy:0.999988, AUC:1.000000\n",
            "Epoch:134, d_loss:0.003984, g_loss:2.479939, accuracy:0.999786, AUC:0.999999\n",
            "Epoch:135, d_loss:0.006777, g_loss:15.133845, accuracy:0.999976, AUC:1.000000\n",
            "Epoch:136, d_loss:0.102013, g_loss:15.467174, accuracy:0.999976, AUC:1.000000\n",
            "Epoch:137, d_loss:0.000853, g_loss:25.652077, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:138, d_loss:0.000653, g_loss:18.281677, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:139, d_loss:0.000491, g_loss:16.896347, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:140, d_loss:0.000406, g_loss:13.577232, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:141, d_loss:0.000375, g_loss:13.864619, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:142, d_loss:0.000347, g_loss:13.122432, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:143, d_loss:0.000333, g_loss:12.831306, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:144, d_loss:0.000336, g_loss:12.019430, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:145, d_loss:0.000350, g_loss:11.326941, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:146, d_loss:0.000364, g_loss:10.827462, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:147, d_loss:0.000383, g_loss:10.370592, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:148, d_loss:0.000426, g_loss:9.812759, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:149, d_loss:0.000494, g_loss:9.137953, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:150, d_loss:0.000589, g_loss:8.928527, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:151, d_loss:0.000615, g_loss:9.112491, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:152, d_loss:0.000577, g_loss:9.639879, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:153, d_loss:0.000643, g_loss:9.868690, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:154, d_loss:0.000732, g_loss:9.234253, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:155, d_loss:0.000860, g_loss:8.892273, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:156, d_loss:0.001164, g_loss:8.298432, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:157, d_loss:0.002943, g_loss:8.769359, accuracy:0.999988, AUC:1.000000\n",
            "Epoch:158, d_loss:0.030318, g_loss:8.328786, accuracy:0.987619, AUC:0.999286\n",
            "Epoch:159, d_loss:0.008944, g_loss:7.730886, accuracy:0.997357, AUC:0.999966\n",
            "Epoch:160, d_loss:0.006618, g_loss:10.637219, accuracy:0.998702, AUC:0.999759\n",
            "Epoch:161, d_loss:0.005894, g_loss:8.966945, accuracy:0.997214, AUC:0.999904\n",
            "Epoch:162, d_loss:0.007238, g_loss:7.507716, accuracy:0.998726, AUC:0.999999\n",
            "Epoch:163, d_loss:0.002532, g_loss:10.770393, accuracy:0.999595, AUC:1.000000\n",
            "Epoch:164, d_loss:0.006864, g_loss:6.925665, accuracy:0.999667, AUC:1.000000\n",
            "Epoch:165, d_loss:0.002021, g_loss:8.390028, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:166, d_loss:0.007447, g_loss:6.878081, accuracy:0.997631, AUC:0.999952\n",
            "Epoch:167, d_loss:0.005012, g_loss:8.256031, accuracy:0.997964, AUC:0.999999\n",
            "Epoch:168, d_loss:0.014357, g_loss:19.504581, accuracy:0.998286, AUC:1.000000\n",
            "Epoch:169, d_loss:0.000697, g_loss:24.510441, accuracy:0.999905, AUC:1.000000\n",
            "Epoch:170, d_loss:0.005409, g_loss:10.265128, accuracy:0.999655, AUC:1.000000\n",
            "Epoch:171, d_loss:0.003307, g_loss:7.290344, accuracy:0.999988, AUC:1.000000\n",
            "Epoch:172, d_loss:0.001987, g_loss:7.521238, accuracy:0.999988, AUC:1.000000\n",
            "Epoch:173, d_loss:0.003282, g_loss:7.146399, accuracy:0.999786, AUC:1.000000\n",
            "Epoch:174, d_loss:0.003506, g_loss:7.540265, accuracy:1.000000, AUC:1.000000\n",
            "Epoch:175, d_loss:0.002269, g_loss:6.929414, accuracy:0.999988, AUC:1.000000\n",
            "Epoch:176, d_loss:0.028775, g_loss:5.036695, accuracy:0.998940, AUC:0.999965\n",
            "Epoch:177, d_loss:0.007981, g_loss:20.390158, accuracy:0.999988, AUC:1.000000\n",
            "Epoch:178, d_loss:0.003007, g_loss:11.930580, accuracy:0.999155, AUC:1.000000\n",
            "Epoch:179, d_loss:0.004183, g_loss:7.119794, accuracy:0.988024, AUC:0.999353\n",
            "Epoch:180, d_loss:0.014740, g_loss:9.048717, accuracy:0.985655, AUC:0.999556\n",
            "Epoch:181, d_loss:0.009089, g_loss:7.762605, accuracy:0.999155, AUC:1.000000\n",
            "Epoch:182, d_loss:0.014087, g_loss:9.635198, accuracy:0.990786, AUC:0.999884\n",
            "Epoch:183, d_loss:0.005887, g_loss:24.460400, accuracy:0.999060, AUC:0.999995\n",
            "Epoch:184, d_loss:0.037864, g_loss:9.812503, accuracy:0.981810, AUC:0.997579\n",
            "Epoch:185, d_loss:0.016567, g_loss:8.468291, accuracy:0.990667, AUC:0.998836\n",
            "Epoch:186, d_loss:0.005202, g_loss:7.978413, accuracy:0.997000, AUC:0.999992\n",
            "Epoch:187, d_loss:0.005059, g_loss:7.707367, accuracy:0.999024, AUC:0.999991\n",
            "Epoch:188, d_loss:0.013227, g_loss:7.707743, accuracy:0.991298, AUC:0.999748\n",
            "Epoch:189, d_loss:0.023263, g_loss:21.447733, accuracy:0.998857, AUC:1.000000\n",
            "Epoch:190, d_loss:0.019655, g_loss:9.724117, accuracy:0.983488, AUC:0.999327\n",
            "Epoch:191, d_loss:0.010536, g_loss:8.332879, accuracy:0.996357, AUC:0.999791\n",
            "Epoch:192, d_loss:0.005282, g_loss:8.522756, accuracy:0.995429, AUC:0.999996\n",
            "Epoch:193, d_loss:0.004720, g_loss:10.065966, accuracy:0.998083, AUC:0.999977\n",
            "Epoch:194, d_loss:0.011948, g_loss:7.524122, accuracy:0.994845, AUC:0.999978\n",
            "Epoch:195, d_loss:0.011566, g_loss:6.999834, accuracy:0.983440, AUC:0.999928\n",
            "Epoch:196, d_loss:0.010592, g_loss:7.834226, accuracy:0.993226, AUC:0.999975\n",
            "Epoch:197, d_loss:0.009643, g_loss:7.021447, accuracy:0.991536, AUC:0.999817\n",
            "Epoch:198, d_loss:0.005650, g_loss:7.481827, accuracy:0.993274, AUC:0.999982\n",
            "Epoch:199, d_loss:0.013609, g_loss:7.785896, accuracy:0.996988, AUC:0.999999\n",
            "Epoch:200, d_loss:0.010637, g_loss:8.118386, accuracy:0.995095, AUC:0.999994\n",
            "Epoch:201, d_loss:0.003893, g_loss:7.807027, accuracy:0.999679, AUC:1.000000\n",
            "Epoch:202, d_loss:0.008574, g_loss:6.683678, accuracy:0.981607, AUC:0.999620\n",
            "Epoch:203, d_loss:0.007446, g_loss:7.601855, accuracy:0.993571, AUC:0.999924\n",
            "Epoch:204, d_loss:0.012251, g_loss:6.660789, accuracy:0.995619, AUC:0.999742\n",
            "Epoch:205, d_loss:0.010469, g_loss:6.237270, accuracy:0.993667, AUC:0.999959\n",
            "Epoch:206, d_loss:0.009001, g_loss:6.779314, accuracy:0.991869, AUC:0.999972\n",
            "Epoch:207, d_loss:0.008062, g_loss:8.488515, accuracy:0.989024, AUC:0.999921\n",
            "Epoch:208, d_loss:0.042246, g_loss:11.432625, accuracy:0.998000, AUC:1.000000\n",
            "Epoch:209, d_loss:0.008378, g_loss:8.350813, accuracy:0.994036, AUC:0.999836\n",
            "Epoch:210, d_loss:0.026335, g_loss:6.741111, accuracy:0.974702, AUC:0.997448\n",
            "Epoch:211, d_loss:0.011124, g_loss:8.277722, accuracy:0.993250, AUC:0.999955\n",
            "Epoch:212, d_loss:0.006340, g_loss:7.879579, accuracy:0.998464, AUC:0.999999\n",
            "Epoch:213, d_loss:0.037683, g_loss:10.844766, accuracy:0.995940, AUC:1.000000\n",
            "Epoch:214, d_loss:0.001345, g_loss:10.726179, accuracy:0.999536, AUC:0.999999\n",
            "Epoch:215, d_loss:0.019222, g_loss:7.274048, accuracy:0.988321, AUC:0.999592\n",
            "Epoch:216, d_loss:0.011445, g_loss:7.736350, accuracy:0.976917, AUC:0.998251\n",
            "Epoch:217, d_loss:0.030502, g_loss:7.378974, accuracy:0.985357, AUC:0.998052\n",
            "Epoch:218, d_loss:0.013098, g_loss:8.310739, accuracy:0.995179, AUC:0.999857\n",
            "Epoch:219, d_loss:0.007234, g_loss:8.374342, accuracy:0.990798, AUC:0.999985\n",
            "Epoch:220, d_loss:0.011660, g_loss:7.509064, accuracy:0.995500, AUC:0.999794\n",
            "Epoch:221, d_loss:0.005934, g_loss:6.015052, accuracy:0.992929, AUC:0.999989\n",
            "Epoch:222, d_loss:0.063896, g_loss:9.843742, accuracy:0.978583, AUC:0.998153\n",
            "Epoch:223, d_loss:0.064087, g_loss:9.080544, accuracy:0.997762, AUC:0.999960\n",
            "Epoch:224, d_loss:0.008177, g_loss:8.107352, accuracy:0.994619, AUC:0.999697\n",
            "Epoch:225, d_loss:0.035919, g_loss:7.222429, accuracy:0.979095, AUC:0.996271\n",
            "Epoch:226, d_loss:0.014113, g_loss:9.648040, accuracy:0.989738, AUC:0.998180\n",
            "Epoch:227, d_loss:0.005070, g_loss:9.585671, accuracy:0.993679, AUC:0.999814\n",
            "Epoch:228, d_loss:0.004553, g_loss:10.167983, accuracy:0.965476, AUC:0.999967\n",
            "Epoch:229, d_loss:0.015479, g_loss:8.564119, accuracy:0.973429, AUC:0.996888\n",
            "Epoch:230, d_loss:0.017408, g_loss:8.605128, accuracy:0.994274, AUC:0.999701\n",
            "Epoch:231, d_loss:0.012231, g_loss:9.528442, accuracy:0.994012, AUC:0.999908\n",
            "Epoch:232, d_loss:0.029751, g_loss:8.011538, accuracy:0.990524, AUC:0.999432\n",
            "Epoch:233, d_loss:0.013648, g_loss:7.862870, accuracy:0.992833, AUC:0.999947\n",
            "Epoch:234, d_loss:0.019807, g_loss:8.345644, accuracy:0.945536, AUC:0.999968\n",
            "Epoch:235, d_loss:0.048815, g_loss:9.730035, accuracy:0.981560, AUC:0.995748\n",
            "Epoch:236, d_loss:0.014525, g_loss:8.954707, accuracy:0.990536, AUC:0.999330\n",
            "Epoch:237, d_loss:0.007191, g_loss:9.372566, accuracy:0.993083, AUC:0.999992\n",
            "Epoch:238, d_loss:0.013108, g_loss:7.388627, accuracy:0.990679, AUC:0.999507\n",
            "Epoch:239, d_loss:0.037877, g_loss:7.779954, accuracy:0.956952, AUC:0.993508\n",
            "Epoch:240, d_loss:0.023932, g_loss:7.829032, accuracy:0.988333, AUC:0.998938\n",
            "Epoch:241, d_loss:0.015902, g_loss:7.308448, accuracy:0.995143, AUC:0.999932\n",
            "Epoch:242, d_loss:0.018782, g_loss:7.443404, accuracy:0.981488, AUC:0.999598\n",
            "Epoch:243, d_loss:0.011893, g_loss:9.127764, accuracy:0.993262, AUC:0.999868\n",
            "Epoch:244, d_loss:0.019484, g_loss:9.211962, accuracy:0.981405, AUC:0.999264\n",
            "Epoch:245, d_loss:0.016080, g_loss:7.965355, accuracy:0.954583, AUC:0.999862\n",
            "Epoch:246, d_loss:0.021640, g_loss:8.103930, accuracy:0.978321, AUC:0.999572\n",
            "Epoch:247, d_loss:0.021223, g_loss:8.033664, accuracy:0.994274, AUC:0.999725\n",
            "Epoch:248, d_loss:0.015464, g_loss:8.339471, accuracy:0.989774, AUC:0.999822\n",
            "Epoch:249, d_loss:0.015519, g_loss:8.501485, accuracy:0.994000, AUC:0.999984\n",
            "Epoch:250, d_loss:0.012206, g_loss:8.885768, accuracy:0.993583, AUC:0.999841\n",
            "Epoch:251, d_loss:0.013888, g_loss:9.523675, accuracy:0.995607, AUC:0.999696\n",
            "Epoch:252, d_loss:0.023302, g_loss:9.771508, accuracy:0.993952, AUC:0.999588\n",
            "Epoch:253, d_loss:0.019606, g_loss:9.291445, accuracy:0.980440, AUC:0.999316\n",
            "Epoch:254, d_loss:0.027005, g_loss:9.215691, accuracy:0.996750, AUC:0.999695\n",
            "Epoch:255, d_loss:0.013026, g_loss:7.075552, accuracy:0.997381, AUC:0.999944\n",
            "Epoch:256, d_loss:0.009740, g_loss:8.825529, accuracy:0.996821, AUC:0.999991\n",
            "Epoch:257, d_loss:0.010473, g_loss:8.224457, accuracy:0.987810, AUC:0.999658\n",
            "Epoch:258, d_loss:0.014640, g_loss:7.904384, accuracy:0.996274, AUC:0.999951\n",
            "Epoch:259, d_loss:0.026619, g_loss:8.971140, accuracy:0.993488, AUC:0.999679\n",
            "Epoch:260, d_loss:0.019409, g_loss:7.933906, accuracy:0.989274, AUC:0.998890\n",
            "Epoch:261, d_loss:0.006784, g_loss:8.348595, accuracy:0.999107, AUC:0.999987\n",
            "Epoch:262, d_loss:0.014140, g_loss:8.414931, accuracy:0.922702, AUC:0.999959\n",
            "Epoch:263, d_loss:0.025411, g_loss:7.645638, accuracy:0.978512, AUC:0.997565\n",
            "Epoch:264, d_loss:0.017284, g_loss:12.972386, accuracy:0.986690, AUC:0.999454\n",
            "Epoch:265, d_loss:0.007427, g_loss:8.113940, accuracy:0.998298, AUC:0.999950\n",
            "Epoch:266, d_loss:0.038194, g_loss:7.886663, accuracy:0.997869, AUC:0.999408\n",
            "Epoch:267, d_loss:0.031913, g_loss:11.265162, accuracy:0.937869, AUC:0.999961\n",
            "Epoch:268, d_loss:0.029538, g_loss:9.752529, accuracy:0.983500, AUC:0.998640\n",
            "Epoch:269, d_loss:0.005445, g_loss:8.254306, accuracy:0.996107, AUC:0.999824\n",
            "Epoch:270, d_loss:0.003767, g_loss:8.140269, accuracy:0.987583, AUC:0.999695\n",
            "Epoch:271, d_loss:0.018848, g_loss:8.601377, accuracy:0.996964, AUC:0.999820\n",
            "Epoch:272, d_loss:0.029906, g_loss:7.026208, accuracy:0.996405, AUC:0.999847\n",
            "Epoch:273, d_loss:0.006370, g_loss:7.828695, accuracy:0.998929, AUC:0.999994\n",
            "Epoch:274, d_loss:0.002827, g_loss:9.431748, accuracy:0.999702, AUC:1.000000\n",
            "Epoch:275, d_loss:0.002989, g_loss:9.617216, accuracy:0.998381, AUC:0.999818\n",
            "Epoch:276, d_loss:0.028773, g_loss:7.944379, accuracy:0.961964, AUC:0.993716\n",
            "Epoch:277, d_loss:0.032028, g_loss:7.582527, accuracy:0.992048, AUC:0.999663\n",
            "Epoch:278, d_loss:0.030166, g_loss:6.704041, accuracy:0.966000, AUC:0.995780\n",
            "Epoch:279, d_loss:0.029850, g_loss:7.835427, accuracy:0.985929, AUC:0.998868\n",
            "Epoch:280, d_loss:0.032608, g_loss:9.479884, accuracy:0.993190, AUC:0.999578\n",
            "Epoch:281, d_loss:0.018013, g_loss:8.595718, accuracy:0.994143, AUC:0.998854\n",
            "Epoch:282, d_loss:0.028358, g_loss:8.460500, accuracy:0.982452, AUC:0.998040\n",
            "Epoch:283, d_loss:0.020228, g_loss:8.122204, accuracy:0.986405, AUC:0.999364\n",
            "Epoch:284, d_loss:0.019505, g_loss:8.325259, accuracy:0.994476, AUC:0.999653\n",
            "Epoch:285, d_loss:0.031807, g_loss:7.932774, accuracy:0.965643, AUC:0.997971\n",
            "Epoch:286, d_loss:0.042001, g_loss:7.889454, accuracy:0.983286, AUC:0.999413\n",
            "Epoch:287, d_loss:0.040674, g_loss:7.340698, accuracy:0.982381, AUC:0.997351\n",
            "Epoch:288, d_loss:0.072344, g_loss:7.452885, accuracy:0.985310, AUC:0.999038\n",
            "Epoch:289, d_loss:0.013566, g_loss:8.082832, accuracy:0.990262, AUC:0.999988\n",
            "Epoch:290, d_loss:0.034181, g_loss:9.194024, accuracy:0.982119, AUC:0.998824\n",
            "Epoch:291, d_loss:0.067327, g_loss:8.528127, accuracy:0.932298, AUC:0.987455\n",
            "Epoch:292, d_loss:0.074170, g_loss:8.809826, accuracy:0.990226, AUC:0.998321\n",
            "Epoch:293, d_loss:0.038512, g_loss:9.017768, accuracy:0.939667, AUC:0.985603\n",
            "Epoch:294, d_loss:0.023256, g_loss:10.131371, accuracy:0.981476, AUC:0.995019\n",
            "Epoch:295, d_loss:0.030469, g_loss:7.145674, accuracy:0.977917, AUC:0.994761\n",
            "Epoch:296, d_loss:0.046029, g_loss:8.686654, accuracy:0.979143, AUC:0.997576\n",
            "Epoch:297, d_loss:0.061682, g_loss:9.038218, accuracy:0.993321, AUC:0.999994\n",
            "Epoch:298, d_loss:0.053562, g_loss:9.997489, accuracy:0.963179, AUC:0.989019\n",
            "Epoch:299, d_loss:0.037621, g_loss:7.979520, accuracy:0.985476, AUC:0.998862\n",
            "Epoch:300, d_loss:0.028262, g_loss:8.467190, accuracy:0.982321, AUC:0.996679\n",
            "Epoch:301, d_loss:0.023308, g_loss:7.967978, accuracy:0.988024, AUC:0.999279\n",
            "Epoch:302, d_loss:0.014588, g_loss:8.373480, accuracy:0.969024, AUC:0.998543\n",
            "Epoch:303, d_loss:0.034557, g_loss:7.752683, accuracy:0.986500, AUC:0.998584\n",
            "Epoch:304, d_loss:0.024331, g_loss:9.326681, accuracy:0.988440, AUC:0.999136\n",
            "Epoch:305, d_loss:0.016058, g_loss:8.771886, accuracy:0.983143, AUC:0.999048\n",
            "Epoch:306, d_loss:0.026342, g_loss:9.727437, accuracy:0.992738, AUC:0.999596\n",
            "Epoch:307, d_loss:0.019045, g_loss:7.017488, accuracy:0.959964, AUC:0.996929\n",
            "Epoch:308, d_loss:0.080762, g_loss:9.965319, accuracy:0.985714, AUC:0.999188\n",
            "Epoch:309, d_loss:0.023673, g_loss:8.149934, accuracy:0.994167, AUC:0.999242\n",
            "Epoch:310, d_loss:0.017748, g_loss:8.367734, accuracy:0.973512, AUC:0.997168\n",
            "Epoch:311, d_loss:0.029206, g_loss:8.043355, accuracy:0.987131, AUC:0.998201\n",
            "Epoch:312, d_loss:0.029597, g_loss:6.768499, accuracy:0.976821, AUC:0.997391\n",
            "Epoch:313, d_loss:0.020967, g_loss:7.189556, accuracy:0.992536, AUC:0.999482\n",
            "Epoch:314, d_loss:0.025522, g_loss:7.882865, accuracy:0.980762, AUC:0.999195\n",
            "Epoch:315, d_loss:0.010093, g_loss:20.352343, accuracy:0.999702, AUC:1.000000\n",
            "Epoch:316, d_loss:0.000041, g_loss:27.631021, accuracy:0.999833, AUC:1.000000\n",
            "Epoch:317, d_loss:0.000143, g_loss:26.207403, accuracy:0.999810, AUC:1.000000\n",
            "Epoch:318, d_loss:0.007499, g_loss:11.066604, accuracy:0.990940, AUC:0.999998\n",
            "Epoch:319, d_loss:0.037431, g_loss:7.287545, accuracy:0.995119, AUC:0.999852\n",
            "Epoch:320, d_loss:0.051385, g_loss:7.272079, accuracy:0.891429, AUC:0.998709\n",
            "Epoch:321, d_loss:0.070517, g_loss:8.724190, accuracy:0.976393, AUC:0.995615\n",
            "Epoch:322, d_loss:0.059765, g_loss:7.221943, accuracy:0.988071, AUC:0.999611\n",
            "Epoch:323, d_loss:0.042860, g_loss:7.674899, accuracy:0.985940, AUC:0.999062\n",
            "Epoch:324, d_loss:0.035371, g_loss:7.474679, accuracy:0.918774, AUC:0.993124\n",
            "Epoch:325, d_loss:0.050710, g_loss:6.960166, accuracy:0.974000, AUC:0.997119\n",
            "Epoch:326, d_loss:0.046646, g_loss:7.675020, accuracy:0.970774, AUC:0.993986\n",
            "Epoch:327, d_loss:0.030251, g_loss:7.096562, accuracy:0.985167, AUC:0.998608\n",
            "Epoch:328, d_loss:0.019162, g_loss:7.668972, accuracy:0.978869, AUC:0.998176\n",
            "Epoch:329, d_loss:0.021489, g_loss:7.496572, accuracy:0.990214, AUC:0.999373\n",
            "Epoch:330, d_loss:0.028141, g_loss:6.729934, accuracy:0.978012, AUC:0.999592\n",
            "Epoch:331, d_loss:0.028590, g_loss:7.853732, accuracy:0.964857, AUC:0.997289\n",
            "Epoch:332, d_loss:0.036497, g_loss:7.114285, accuracy:0.984274, AUC:0.998971\n",
            "Epoch:333, d_loss:0.034087, g_loss:7.200926, accuracy:0.984524, AUC:0.997790\n",
            "Epoch:334, d_loss:0.038567, g_loss:7.248972, accuracy:0.921060, AUC:0.997975\n",
            "Epoch:335, d_loss:0.016917, g_loss:8.794135, accuracy:0.991060, AUC:0.999740\n",
            "Epoch:336, d_loss:0.059267, g_loss:7.152276, accuracy:0.962024, AUC:0.997906\n",
            "Epoch:337, d_loss:0.041074, g_loss:9.131868, accuracy:0.989107, AUC:0.999328\n",
            "Epoch:338, d_loss:0.019612, g_loss:8.901032, accuracy:0.983298, AUC:0.999026\n",
            "Epoch:339, d_loss:0.032550, g_loss:7.889473, accuracy:0.995119, AUC:0.998842\n",
            "Epoch:340, d_loss:0.034907, g_loss:6.836529, accuracy:0.945369, AUC:0.995687\n",
            "Epoch:341, d_loss:0.033260, g_loss:8.240212, accuracy:0.987357, AUC:0.998777\n",
            "Epoch:342, d_loss:0.036349, g_loss:7.043886, accuracy:0.973310, AUC:0.994577\n",
            "Epoch:343, d_loss:0.025278, g_loss:7.532418, accuracy:0.983702, AUC:0.998783\n",
            "Epoch:344, d_loss:0.030083, g_loss:7.882313, accuracy:0.962464, AUC:0.997772\n",
            "Epoch:345, d_loss:0.041246, g_loss:9.780636, accuracy:0.996321, AUC:0.999825\n",
            "Epoch:346, d_loss:0.015015, g_loss:9.109184, accuracy:0.983333, AUC:0.999247\n",
            "Epoch:347, d_loss:0.059444, g_loss:8.415944, accuracy:0.978524, AUC:0.998666\n",
            "Epoch:348, d_loss:0.028755, g_loss:9.030155, accuracy:0.978786, AUC:0.994992\n",
            "Epoch:349, d_loss:0.058715, g_loss:8.952195, accuracy:0.985893, AUC:0.999557\n",
            "Epoch:350, d_loss:0.021815, g_loss:8.689997, accuracy:0.986214, AUC:0.999486\n",
            "Epoch:351, d_loss:0.051738, g_loss:7.911383, accuracy:0.973774, AUC:0.997622\n",
            "Epoch:352, d_loss:0.024860, g_loss:8.098206, accuracy:0.994929, AUC:0.999571\n",
            "Epoch:353, d_loss:0.043628, g_loss:7.087537, accuracy:0.980107, AUC:0.997205\n",
            "Epoch:354, d_loss:0.051844, g_loss:8.176787, accuracy:0.980560, AUC:0.998463\n",
            "Epoch:355, d_loss:0.050956, g_loss:7.409515, accuracy:0.976250, AUC:0.994560\n",
            "Epoch:356, d_loss:0.049094, g_loss:7.579085, accuracy:0.989214, AUC:0.999702\n",
            "Epoch:357, d_loss:0.024763, g_loss:8.055937, accuracy:0.988357, AUC:0.999297\n",
            "Epoch:358, d_loss:0.042111, g_loss:6.603939, accuracy:0.986333, AUC:0.998600\n",
            "Epoch:359, d_loss:0.051505, g_loss:7.759943, accuracy:0.969905, AUC:0.997970\n",
            "Epoch:360, d_loss:0.049823, g_loss:7.056273, accuracy:0.978214, AUC:0.998098\n",
            "Epoch:361, d_loss:0.032777, g_loss:6.495485, accuracy:0.979131, AUC:0.997971\n",
            "Epoch:362, d_loss:0.035184, g_loss:7.670053, accuracy:0.951786, AUC:0.999089\n",
            "Epoch:363, d_loss:0.047107, g_loss:7.348819, accuracy:0.972048, AUC:0.998772\n",
            "Epoch:364, d_loss:0.028086, g_loss:6.906621, accuracy:0.979726, AUC:0.997849\n",
            "Epoch:365, d_loss:0.046179, g_loss:7.387367, accuracy:0.976631, AUC:0.995113\n",
            "Epoch:366, d_loss:0.035223, g_loss:7.633599, accuracy:0.995071, AUC:0.999892\n",
            "Epoch:367, d_loss:0.019712, g_loss:7.976307, accuracy:0.993643, AUC:0.999468\n",
            "Epoch:368, d_loss:0.028399, g_loss:7.594371, accuracy:0.936262, AUC:0.997564\n",
            "Epoch:369, d_loss:0.122240, g_loss:7.985427, accuracy:0.977333, AUC:0.993079\n",
            "Epoch:370, d_loss:0.043110, g_loss:7.539257, accuracy:0.969667, AUC:0.996086\n",
            "Epoch:371, d_loss:0.064853, g_loss:7.555315, accuracy:0.966119, AUC:0.994871\n",
            "Epoch:372, d_loss:0.060943, g_loss:7.746618, accuracy:0.977381, AUC:0.998055\n",
            "Epoch:373, d_loss:0.060177, g_loss:7.364567, accuracy:0.983583, AUC:0.996860\n",
            "Epoch:374, d_loss:0.048709, g_loss:6.804252, accuracy:0.966857, AUC:0.988837\n",
            "Epoch:375, d_loss:0.082248, g_loss:6.780117, accuracy:0.920214, AUC:0.976533\n",
            "Epoch:376, d_loss:0.044332, g_loss:7.379600, accuracy:0.968155, AUC:0.995265\n",
            "Epoch:377, d_loss:0.074622, g_loss:7.638171, accuracy:0.971083, AUC:0.996638\n",
            "Epoch:378, d_loss:0.040464, g_loss:7.299634, accuracy:0.966583, AUC:0.994990\n",
            "Epoch:379, d_loss:0.062455, g_loss:7.069280, accuracy:0.963000, AUC:0.991177\n",
            "Epoch:380, d_loss:0.040935, g_loss:8.584915, accuracy:0.960238, AUC:0.991537\n",
            "Epoch:381, d_loss:0.041973, g_loss:7.449458, accuracy:0.988667, AUC:0.999185\n",
            "Epoch:382, d_loss:0.031638, g_loss:7.512520, accuracy:0.971476, AUC:0.993888\n",
            "Epoch:383, d_loss:0.029689, g_loss:7.829439, accuracy:0.981667, AUC:0.996589\n",
            "Epoch:384, d_loss:0.017902, g_loss:7.688593, accuracy:0.968952, AUC:0.996732\n",
            "Epoch:385, d_loss:0.049673, g_loss:7.377401, accuracy:0.988214, AUC:0.997891\n",
            "Epoch:386, d_loss:0.042256, g_loss:7.630802, accuracy:0.970619, AUC:0.997863\n",
            "Epoch:387, d_loss:0.037400, g_loss:7.221717, accuracy:0.991202, AUC:0.998922\n",
            "Epoch:388, d_loss:0.110663, g_loss:7.179703, accuracy:0.988274, AUC:0.998210\n",
            "Epoch:389, d_loss:0.043085, g_loss:7.517071, accuracy:0.988095, AUC:0.997939\n",
            "Epoch:390, d_loss:0.064495, g_loss:8.177770, accuracy:0.986119, AUC:0.996892\n",
            "Epoch:391, d_loss:0.018424, g_loss:9.390574, accuracy:0.983095, AUC:0.996490\n",
            "Epoch:392, d_loss:0.013621, g_loss:9.567555, accuracy:0.981262, AUC:0.997382\n",
            "Epoch:393, d_loss:0.044615, g_loss:6.396253, accuracy:0.972988, AUC:0.996041\n",
            "Epoch:394, d_loss:0.058034, g_loss:7.517962, accuracy:0.929774, AUC:0.995210\n",
            "Epoch:395, d_loss:0.048514, g_loss:6.758358, accuracy:0.945440, AUC:0.997129\n",
            "Epoch:396, d_loss:0.035726, g_loss:8.648467, accuracy:0.959488, AUC:0.994950\n",
            "Epoch:397, d_loss:0.056281, g_loss:7.014056, accuracy:0.960476, AUC:0.998744\n",
            "Epoch:398, d_loss:0.039363, g_loss:7.301325, accuracy:0.975881, AUC:0.994719\n",
            "Epoch:399, d_loss:0.020874, g_loss:7.775387, accuracy:0.994929, AUC:0.999614\n",
            "Epoch:400, d_loss:0.028060, g_loss:8.658055, accuracy:0.923964, AUC:0.995344\n",
            "Epoch:401, d_loss:0.050688, g_loss:8.517504, accuracy:0.982369, AUC:0.997826\n",
            "Epoch:402, d_loss:0.027383, g_loss:9.051965, accuracy:0.980250, AUC:0.996560\n",
            "Epoch:403, d_loss:0.022908, g_loss:8.015673, accuracy:0.979667, AUC:0.996890\n",
            "Epoch:404, d_loss:0.026343, g_loss:7.117795, accuracy:0.995857, AUC:0.999811\n",
            "Epoch:405, d_loss:0.045532, g_loss:6.718112, accuracy:0.961298, AUC:0.995541\n",
            "Epoch:406, d_loss:0.023895, g_loss:7.155975, accuracy:0.986560, AUC:0.999230\n",
            "Epoch:407, d_loss:0.025161, g_loss:8.547478, accuracy:0.992488, AUC:0.999477\n",
            "Epoch:408, d_loss:0.011525, g_loss:11.183181, accuracy:0.981452, AUC:0.997214\n",
            "Epoch:409, d_loss:0.028505, g_loss:7.806681, accuracy:0.980524, AUC:0.998713\n",
            "Epoch:410, d_loss:0.072042, g_loss:9.431270, accuracy:0.972964, AUC:0.991580\n",
            "Epoch:411, d_loss:0.078164, g_loss:9.490560, accuracy:0.984452, AUC:0.999922\n",
            "Epoch:412, d_loss:0.013368, g_loss:8.064778, accuracy:0.988333, AUC:0.998312\n",
            "Epoch:413, d_loss:0.037061, g_loss:8.291028, accuracy:0.975607, AUC:0.997040\n",
            "Epoch:414, d_loss:0.043028, g_loss:7.255416, accuracy:0.956952, AUC:0.996369\n",
            "Epoch:415, d_loss:0.033681, g_loss:7.298340, accuracy:0.984274, AUC:0.997553\n",
            "Epoch:416, d_loss:0.049983, g_loss:6.509168, accuracy:0.949262, AUC:0.994640\n",
            "Epoch:417, d_loss:0.048173, g_loss:7.317472, accuracy:0.924821, AUC:0.994220\n",
            "Epoch:418, d_loss:0.047954, g_loss:7.028399, accuracy:0.964964, AUC:0.996454\n",
            "Epoch:419, d_loss:0.030404, g_loss:7.071565, accuracy:0.982679, AUC:0.997229\n",
            "Epoch:420, d_loss:0.036780, g_loss:7.990479, accuracy:0.978714, AUC:0.996101\n",
            "Epoch:421, d_loss:0.047874, g_loss:7.500582, accuracy:0.959083, AUC:0.997817\n",
            "Epoch:422, d_loss:0.038755, g_loss:7.153849, accuracy:0.979893, AUC:0.999002\n",
            "Epoch:423, d_loss:0.047737, g_loss:7.562199, accuracy:0.852988, AUC:0.996160\n",
            "Epoch:424, d_loss:0.058114, g_loss:6.933017, accuracy:0.979536, AUC:0.994968\n",
            "Epoch:425, d_loss:0.051436, g_loss:6.744367, accuracy:0.969512, AUC:0.991897\n",
            "Epoch:426, d_loss:0.033071, g_loss:7.569941, accuracy:0.977833, AUC:0.994681\n",
            "Epoch:427, d_loss:0.028829, g_loss:7.851367, accuracy:0.983345, AUC:0.998100\n",
            "Epoch:428, d_loss:0.026629, g_loss:8.858194, accuracy:0.979524, AUC:0.998811\n",
            "Epoch:429, d_loss:0.019065, g_loss:8.048680, accuracy:0.969321, AUC:0.999331\n",
            "Epoch:430, d_loss:0.034593, g_loss:8.061857, accuracy:0.982500, AUC:0.996241\n",
            "Epoch:431, d_loss:0.043263, g_loss:7.371031, accuracy:0.960595, AUC:0.998333\n",
            "Epoch:432, d_loss:0.054065, g_loss:7.062860, accuracy:0.976905, AUC:0.998826\n",
            "Epoch:433, d_loss:0.043577, g_loss:7.377845, accuracy:0.981988, AUC:0.998693\n",
            "Epoch:434, d_loss:0.027780, g_loss:7.209100, accuracy:0.984143, AUC:0.999136\n",
            "Epoch:435, d_loss:0.021034, g_loss:7.223472, accuracy:0.982429, AUC:0.998662\n",
            "Epoch:436, d_loss:0.026814, g_loss:6.604076, accuracy:0.971333, AUC:0.994536\n",
            "Epoch:437, d_loss:0.047916, g_loss:6.642671, accuracy:0.976226, AUC:0.998697\n",
            "Epoch:438, d_loss:0.070311, g_loss:7.172475, accuracy:0.942988, AUC:0.993932\n",
            "Epoch:439, d_loss:0.063848, g_loss:6.681370, accuracy:0.959667, AUC:0.993554\n",
            "Epoch:440, d_loss:0.051883, g_loss:7.290872, accuracy:0.987107, AUC:0.998820\n",
            "Epoch:441, d_loss:0.033856, g_loss:6.472396, accuracy:0.967512, AUC:0.994978\n",
            "Epoch:442, d_loss:0.047778, g_loss:7.363422, accuracy:0.955357, AUC:0.993010\n",
            "Epoch:443, d_loss:0.034698, g_loss:7.085265, accuracy:0.970417, AUC:0.996853\n",
            "Epoch:444, d_loss:0.052736, g_loss:6.852830, accuracy:0.973286, AUC:0.996850\n",
            "Epoch:445, d_loss:0.055627, g_loss:7.105649, accuracy:0.979583, AUC:0.995823\n",
            "Epoch:446, d_loss:0.032360, g_loss:8.018526, accuracy:0.972393, AUC:0.995716\n",
            "Epoch:447, d_loss:0.040471, g_loss:6.970705, accuracy:0.968607, AUC:0.994940\n",
            "Epoch:448, d_loss:0.027502, g_loss:7.427267, accuracy:0.973155, AUC:0.996582\n",
            "Epoch:449, d_loss:0.035829, g_loss:7.019819, accuracy:0.970524, AUC:0.997958\n",
            "Epoch:450, d_loss:0.084323, g_loss:6.728148, accuracy:0.974024, AUC:0.996138\n",
            "Epoch:451, d_loss:0.054576, g_loss:6.319545, accuracy:0.927417, AUC:0.987935\n",
            "Epoch:452, d_loss:0.048845, g_loss:7.302779, accuracy:0.953452, AUC:0.992935\n",
            "Epoch:453, d_loss:0.042687, g_loss:6.902308, accuracy:0.977940, AUC:0.997317\n",
            "Epoch:454, d_loss:0.040545, g_loss:7.547904, accuracy:0.931119, AUC:0.996103\n",
            "Epoch:455, d_loss:0.071869, g_loss:8.008040, accuracy:0.946619, AUC:0.979221\n",
            "Epoch:456, d_loss:0.056600, g_loss:6.772064, accuracy:0.958417, AUC:0.994771\n",
            "Epoch:457, d_loss:0.044692, g_loss:7.072360, accuracy:0.960762, AUC:0.998255\n",
            "Epoch:458, d_loss:0.044232, g_loss:7.175193, accuracy:0.968512, AUC:0.993249\n",
            "Epoch:459, d_loss:0.043040, g_loss:7.332137, accuracy:0.945321, AUC:0.994134\n",
            "Epoch:460, d_loss:0.049150, g_loss:6.820107, accuracy:0.972012, AUC:0.995026\n",
            "Epoch:461, d_loss:0.050608, g_loss:7.133036, accuracy:0.964333, AUC:0.993850\n",
            "Epoch:462, d_loss:0.047542, g_loss:7.417497, accuracy:0.942417, AUC:0.994435\n",
            "Epoch:463, d_loss:0.056884, g_loss:6.971847, accuracy:0.927393, AUC:0.996316\n",
            "Epoch:464, d_loss:0.046048, g_loss:7.130607, accuracy:0.961345, AUC:0.996874\n",
            "Epoch:465, d_loss:0.033864, g_loss:7.748389, accuracy:0.968405, AUC:0.996081\n",
            "Epoch:466, d_loss:0.051539, g_loss:7.356656, accuracy:0.946821, AUC:0.993214\n",
            "Epoch:467, d_loss:0.049684, g_loss:7.110685, accuracy:0.971964, AUC:0.992240\n",
            "Epoch:468, d_loss:0.036362, g_loss:7.805546, accuracy:0.991202, AUC:0.999418\n",
            "Epoch:469, d_loss:0.034888, g_loss:7.868019, accuracy:0.961393, AUC:0.992687\n",
            "Epoch:470, d_loss:0.022728, g_loss:7.873635, accuracy:0.940405, AUC:0.997678\n",
            "Epoch:471, d_loss:0.061787, g_loss:8.259572, accuracy:0.962071, AUC:0.998773\n",
            "Epoch:472, d_loss:0.059628, g_loss:8.240880, accuracy:0.771667, AUC:0.997985\n",
            "Epoch:473, d_loss:0.071647, g_loss:7.801879, accuracy:0.961786, AUC:0.985382\n",
            "Epoch:474, d_loss:0.057944, g_loss:6.583462, accuracy:0.959179, AUC:0.992195\n",
            "Epoch:475, d_loss:0.055876, g_loss:7.246300, accuracy:0.947024, AUC:0.993261\n",
            "Epoch:476, d_loss:0.040870, g_loss:6.472131, accuracy:0.980190, AUC:0.996160\n",
            "Epoch:477, d_loss:0.051995, g_loss:7.041739, accuracy:0.977940, AUC:0.997736\n",
            "Epoch:478, d_loss:0.023602, g_loss:7.016246, accuracy:0.951143, AUC:0.987215\n",
            "Epoch:479, d_loss:0.043033, g_loss:6.626793, accuracy:0.985214, AUC:0.997979\n",
            "Epoch:480, d_loss:0.034193, g_loss:7.996087, accuracy:0.979619, AUC:0.995570\n",
            "Epoch:481, d_loss:0.022632, g_loss:8.340848, accuracy:0.980952, AUC:0.999740\n",
            "Epoch:482, d_loss:0.020011, g_loss:8.338760, accuracy:0.985679, AUC:0.997849\n",
            "Epoch:483, d_loss:0.029292, g_loss:7.949506, accuracy:0.966833, AUC:0.998256\n",
            "Epoch:484, d_loss:0.024132, g_loss:7.355899, accuracy:0.989214, AUC:0.999964\n",
            "Epoch:485, d_loss:0.014717, g_loss:8.361043, accuracy:0.993798, AUC:0.999172\n",
            "Epoch:486, d_loss:0.020785, g_loss:8.064955, accuracy:0.966512, AUC:0.990368\n",
            "Epoch:487, d_loss:0.033500, g_loss:10.703470, accuracy:0.991643, AUC:0.998430\n",
            "Epoch:488, d_loss:0.084036, g_loss:10.669307, accuracy:0.942476, AUC:0.995576\n",
            "Epoch:489, d_loss:0.041524, g_loss:7.829538, accuracy:0.969917, AUC:0.990357\n",
            "Epoch:490, d_loss:0.047490, g_loss:8.961080, accuracy:0.949881, AUC:0.999931\n",
            "Epoch:491, d_loss:0.043398, g_loss:9.590590, accuracy:0.918988, AUC:0.991871\n",
            "Epoch:492, d_loss:0.034164, g_loss:7.250232, accuracy:0.980821, AUC:0.997839\n",
            "Epoch:493, d_loss:0.026504, g_loss:7.580739, accuracy:0.975905, AUC:0.994843\n",
            "Epoch:494, d_loss:0.025723, g_loss:9.085593, accuracy:0.980119, AUC:0.996718\n",
            "Epoch:495, d_loss:0.029924, g_loss:6.222523, accuracy:0.975167, AUC:0.994727\n",
            "Epoch:496, d_loss:0.035848, g_loss:6.679661, accuracy:0.986274, AUC:0.998469\n",
            "Epoch:497, d_loss:0.032817, g_loss:7.090384, accuracy:0.943095, AUC:0.993873\n",
            "Epoch:498, d_loss:0.030351, g_loss:6.788536, accuracy:0.955429, AUC:0.999421\n",
            "Epoch:499, d_loss:0.041981, g_loss:7.049104, accuracy:0.966571, AUC:0.993724\n",
            "Epoch:500, d_loss:0.051357, g_loss:6.820207, accuracy:0.937774, AUC:0.993443\n",
            "Epoch:501, d_loss:0.027220, g_loss:7.182383, accuracy:0.904036, AUC:0.997882\n",
            "Epoch:502, d_loss:0.053242, g_loss:6.703862, accuracy:0.985762, AUC:0.998278\n",
            "Epoch:503, d_loss:0.039691, g_loss:6.881254, accuracy:0.970000, AUC:0.997880\n",
            "Epoch:504, d_loss:0.033647, g_loss:6.612101, accuracy:0.977131, AUC:0.996335\n",
            "Epoch:505, d_loss:0.047454, g_loss:6.686210, accuracy:0.966869, AUC:0.993792\n",
            "Epoch:506, d_loss:0.042535, g_loss:6.900226, accuracy:0.964655, AUC:0.992360\n",
            "Epoch:507, d_loss:0.030104, g_loss:6.485437, accuracy:0.970000, AUC:0.997638\n",
            "Epoch:508, d_loss:0.057788, g_loss:7.261887, accuracy:0.971464, AUC:0.992657\n",
            "Epoch:509, d_loss:0.041132, g_loss:6.684871, accuracy:0.969762, AUC:0.996600\n",
            "Epoch:510, d_loss:0.117299, g_loss:7.498040, accuracy:0.921476, AUC:0.996417\n",
            "Epoch:511, d_loss:0.044757, g_loss:6.814448, accuracy:0.903762, AUC:0.976483\n",
            "Epoch:512, d_loss:0.037676, g_loss:6.715338, accuracy:0.984417, AUC:0.997171\n",
            "Epoch:513, d_loss:0.032284, g_loss:6.999021, accuracy:0.977143, AUC:0.998005\n",
            "Epoch:514, d_loss:0.032631, g_loss:6.455561, accuracy:0.977643, AUC:0.994183\n",
            "Epoch:515, d_loss:0.027521, g_loss:7.294646, accuracy:0.983905, AUC:0.999417\n",
            "Epoch:516, d_loss:0.023492, g_loss:6.913946, accuracy:0.951476, AUC:0.995455\n",
            "Epoch:517, d_loss:0.027941, g_loss:6.684753, accuracy:0.967607, AUC:0.996933\n",
            "Epoch:518, d_loss:0.055150, g_loss:6.536948, accuracy:0.963286, AUC:0.996838\n",
            "Epoch:519, d_loss:0.025784, g_loss:6.477135, accuracy:0.987274, AUC:0.998336\n",
            "Epoch:520, d_loss:0.046535, g_loss:6.679124, accuracy:0.962869, AUC:0.996245\n",
            "Epoch:521, d_loss:0.037547, g_loss:8.228640, accuracy:0.977905, AUC:0.994057\n",
            "Epoch:522, d_loss:0.032813, g_loss:7.640141, accuracy:0.982345, AUC:0.996509\n",
            "Epoch:523, d_loss:0.035666, g_loss:7.001650, accuracy:0.971929, AUC:0.996961\n",
            "Epoch:524, d_loss:0.059493, g_loss:6.600957, accuracy:0.966940, AUC:0.994659\n",
            "Epoch:525, d_loss:0.051152, g_loss:7.183575, accuracy:0.940476, AUC:0.994549\n",
            "Epoch:526, d_loss:0.067145, g_loss:7.005479, accuracy:0.966107, AUC:0.995447\n",
            "Epoch:527, d_loss:0.056001, g_loss:6.629766, accuracy:0.958893, AUC:0.990390\n",
            "Epoch:528, d_loss:0.042421, g_loss:6.983977, accuracy:0.959607, AUC:0.995619\n",
            "Epoch:529, d_loss:0.029832, g_loss:7.054911, accuracy:0.947833, AUC:0.994678\n",
            "Epoch:530, d_loss:0.045493, g_loss:6.587134, accuracy:0.966345, AUC:0.995080\n",
            "Epoch:531, d_loss:0.049146, g_loss:6.896192, accuracy:0.945476, AUC:0.992267\n",
            "Epoch:532, d_loss:0.060723, g_loss:6.675870, accuracy:0.976369, AUC:0.995735\n",
            "Epoch:533, d_loss:0.049198, g_loss:6.426983, accuracy:0.974286, AUC:0.995622\n",
            "Epoch:534, d_loss:0.036723, g_loss:7.057979, accuracy:0.958036, AUC:0.995018\n",
            "Epoch:535, d_loss:0.037251, g_loss:7.418334, accuracy:0.948810, AUC:0.994590\n",
            "Epoch:536, d_loss:0.031286, g_loss:7.256978, accuracy:0.988262, AUC:0.998437\n",
            "Epoch:537, d_loss:0.027056, g_loss:7.346011, accuracy:0.973226, AUC:0.997367\n",
            "Epoch:538, d_loss:0.020886, g_loss:7.994099, accuracy:0.982631, AUC:0.998004\n",
            "Epoch:539, d_loss:0.049167, g_loss:7.977262, accuracy:0.992548, AUC:0.999530\n",
            "Epoch:540, d_loss:0.061596, g_loss:7.406573, accuracy:0.970357, AUC:0.991252\n",
            "Epoch:541, d_loss:0.021799, g_loss:7.262257, accuracy:0.993869, AUC:0.999334\n",
            "Epoch:542, d_loss:0.036999, g_loss:7.058785, accuracy:0.961726, AUC:0.996413\n",
            "Epoch:543, d_loss:0.066660, g_loss:7.283674, accuracy:0.981774, AUC:0.994102\n",
            "Epoch:544, d_loss:0.025923, g_loss:7.231266, accuracy:0.978690, AUC:0.996641\n",
            "Epoch:545, d_loss:0.054449, g_loss:7.632303, accuracy:0.973393, AUC:0.999224\n",
            "Epoch:546, d_loss:0.018681, g_loss:7.728840, accuracy:0.980024, AUC:0.995932\n",
            "Epoch:547, d_loss:0.047193, g_loss:6.455597, accuracy:0.954810, AUC:0.996928\n",
            "Epoch:548, d_loss:0.039394, g_loss:6.369988, accuracy:0.944571, AUC:0.990885\n",
            "Epoch:549, d_loss:0.045266, g_loss:6.681375, accuracy:0.978345, AUC:0.999123\n",
            "Epoch:550, d_loss:0.015852, g_loss:7.699294, accuracy:0.991214, AUC:0.999382\n",
            "Epoch:551, d_loss:0.056016, g_loss:5.955501, accuracy:0.968833, AUC:0.997554\n",
            "Epoch:552, d_loss:0.026502, g_loss:7.382788, accuracy:0.980988, AUC:0.998027\n",
            "Epoch:553, d_loss:0.048175, g_loss:7.584327, accuracy:0.978798, AUC:0.998227\n",
            "Epoch:554, d_loss:0.037461, g_loss:7.696037, accuracy:0.969131, AUC:0.994767\n",
            "Epoch:555, d_loss:0.137474, g_loss:7.269859, accuracy:0.962690, AUC:0.991811\n",
            "Epoch:556, d_loss:0.068881, g_loss:6.759409, accuracy:0.969167, AUC:0.999013\n",
            "Epoch:557, d_loss:0.026906, g_loss:6.951149, accuracy:0.969440, AUC:0.992899\n",
            "Epoch:558, d_loss:0.062173, g_loss:6.915750, accuracy:0.969310, AUC:0.992937\n",
            "Epoch:559, d_loss:0.053039, g_loss:6.604652, accuracy:0.959583, AUC:0.992814\n",
            "Epoch:560, d_loss:0.039361, g_loss:6.667017, accuracy:0.962821, AUC:0.995978\n",
            "Epoch:561, d_loss:0.048620, g_loss:6.550487, accuracy:0.960226, AUC:0.995834\n",
            "Epoch:562, d_loss:0.053013, g_loss:6.775786, accuracy:0.945786, AUC:0.992976\n",
            "Epoch:563, d_loss:0.051396, g_loss:6.794948, accuracy:0.955607, AUC:0.989200\n",
            "Epoch:564, d_loss:0.045450, g_loss:6.702656, accuracy:0.935024, AUC:0.990197\n",
            "Epoch:565, d_loss:0.043565, g_loss:6.735918, accuracy:0.979179, AUC:0.997434\n",
            "Epoch:566, d_loss:0.030330, g_loss:6.731681, accuracy:0.945202, AUC:0.995297\n",
            "Epoch:567, d_loss:0.038246, g_loss:6.780301, accuracy:0.965488, AUC:0.996218\n",
            "Epoch:568, d_loss:0.076197, g_loss:6.917066, accuracy:0.962893, AUC:0.994751\n",
            "Epoch:569, d_loss:0.026500, g_loss:7.467675, accuracy:0.983798, AUC:0.999140\n",
            "Epoch:570, d_loss:0.015435, g_loss:7.945880, accuracy:0.990238, AUC:0.999349\n",
            "Epoch:571, d_loss:0.040079, g_loss:6.825216, accuracy:0.975738, AUC:0.994442\n",
            "Epoch:572, d_loss:0.031472, g_loss:6.888595, accuracy:0.974155, AUC:0.996847\n",
            "Epoch:573, d_loss:0.031782, g_loss:6.579981, accuracy:0.969774, AUC:0.995313\n",
            "Epoch:574, d_loss:0.038420, g_loss:6.393405, accuracy:0.979024, AUC:0.996270\n",
            "Epoch:575, d_loss:0.040382, g_loss:6.357842, accuracy:0.949095, AUC:0.993694\n",
            "Epoch:576, d_loss:0.034907, g_loss:6.980947, accuracy:0.951202, AUC:0.992235\n",
            "Epoch:577, d_loss:0.037356, g_loss:7.120400, accuracy:0.958429, AUC:0.997638\n",
            "Epoch:578, d_loss:0.074649, g_loss:7.371118, accuracy:0.978393, AUC:0.995267\n",
            "Epoch:579, d_loss:0.042085, g_loss:6.900713, accuracy:0.962131, AUC:0.986524\n",
            "Epoch:580, d_loss:0.058530, g_loss:6.530620, accuracy:0.964119, AUC:0.997177\n",
            "Epoch:581, d_loss:0.018869, g_loss:7.033201, accuracy:0.936202, AUC:0.991270\n",
            "Epoch:582, d_loss:0.034102, g_loss:7.304894, accuracy:0.975929, AUC:0.998550\n",
            "Epoch:583, d_loss:0.050889, g_loss:5.971020, accuracy:0.934845, AUC:0.982969\n",
            "Epoch:584, d_loss:0.040925, g_loss:6.368243, accuracy:0.971667, AUC:0.993472\n",
            "Epoch:585, d_loss:0.053875, g_loss:6.972878, accuracy:0.948286, AUC:0.991851\n",
            "Epoch:586, d_loss:0.053355, g_loss:6.758406, accuracy:0.940798, AUC:0.990453\n",
            "Epoch:587, d_loss:0.033149, g_loss:6.707073, accuracy:0.983286, AUC:0.998757\n",
            "Epoch:588, d_loss:0.032128, g_loss:7.096351, accuracy:0.947702, AUC:0.992349\n",
            "Epoch:589, d_loss:0.083698, g_loss:8.190317, accuracy:0.967298, AUC:0.991433\n",
            "Epoch:590, d_loss:0.063891, g_loss:7.575832, accuracy:0.957369, AUC:0.994356\n",
            "Epoch:591, d_loss:0.034230, g_loss:6.881317, accuracy:0.982036, AUC:0.995972\n",
            "Epoch:592, d_loss:0.043864, g_loss:6.665820, accuracy:0.962738, AUC:0.994449\n",
            "Epoch:593, d_loss:0.031847, g_loss:6.760028, accuracy:0.909369, AUC:0.991406\n",
            "Epoch:594, d_loss:0.045753, g_loss:6.249692, accuracy:0.969500, AUC:0.996956\n",
            "Epoch:595, d_loss:0.046646, g_loss:6.610414, accuracy:0.981940, AUC:0.995664\n",
            "Epoch:596, d_loss:0.036664, g_loss:6.506557, accuracy:0.931667, AUC:0.981458\n",
            "Epoch:597, d_loss:0.049112, g_loss:6.359103, accuracy:0.969798, AUC:0.998457\n",
            "Epoch:598, d_loss:0.054774, g_loss:6.612245, accuracy:0.982774, AUC:0.996381\n",
            "Epoch:599, d_loss:0.042307, g_loss:6.187618, accuracy:0.936726, AUC:0.995540\n",
            "Epoch:600, d_loss:0.030738, g_loss:6.611925, accuracy:0.958095, AUC:0.997900\n",
            "Epoch:601, d_loss:0.032022, g_loss:6.704389, accuracy:0.975571, AUC:0.996785\n",
            "Epoch:602, d_loss:0.032293, g_loss:6.704298, accuracy:0.939298, AUC:0.993855\n",
            "Epoch:603, d_loss:0.041958, g_loss:6.536474, accuracy:0.921440, AUC:0.994922\n",
            "Epoch:604, d_loss:0.055810, g_loss:6.042096, accuracy:0.961786, AUC:0.992453\n",
            "Epoch:605, d_loss:0.042012, g_loss:6.685755, accuracy:0.953012, AUC:0.996726\n",
            "Epoch:606, d_loss:0.044247, g_loss:6.225033, accuracy:0.903929, AUC:0.992426\n",
            "Epoch:607, d_loss:0.054086, g_loss:6.607594, accuracy:0.904012, AUC:0.983005\n",
            "Epoch:608, d_loss:0.049299, g_loss:6.167600, accuracy:0.939429, AUC:0.992531\n",
            "Epoch:609, d_loss:0.047241, g_loss:6.566106, accuracy:0.964060, AUC:0.996359\n",
            "Epoch:610, d_loss:0.041985, g_loss:6.916653, accuracy:0.944452, AUC:0.991703\n",
            "Epoch:611, d_loss:0.042559, g_loss:6.741485, accuracy:0.970167, AUC:0.993006\n",
            "Epoch:612, d_loss:0.072087, g_loss:6.902409, accuracy:0.950310, AUC:0.997081\n",
            "Epoch:613, d_loss:0.048290, g_loss:6.469994, accuracy:0.949214, AUC:0.989889\n",
            "Epoch:614, d_loss:0.065914, g_loss:6.420008, accuracy:0.948679, AUC:0.991561\n",
            "Epoch:615, d_loss:0.052968, g_loss:6.768404, accuracy:0.969417, AUC:0.996273\n",
            "Epoch:616, d_loss:0.046186, g_loss:7.151700, accuracy:0.952988, AUC:0.992763\n",
            "Epoch:617, d_loss:0.050930, g_loss:7.712915, accuracy:0.933298, AUC:0.995497\n",
            "Epoch:618, d_loss:0.035236, g_loss:6.711136, accuracy:0.980381, AUC:0.998844\n",
            "Epoch:619, d_loss:0.027471, g_loss:6.682757, accuracy:0.957798, AUC:0.996151\n",
            "Epoch:620, d_loss:0.052791, g_loss:7.128998, accuracy:0.988536, AUC:0.999466\n",
            "Epoch:621, d_loss:0.049471, g_loss:8.128989, accuracy:0.980238, AUC:0.994397\n",
            "Epoch:622, d_loss:0.029878, g_loss:7.769031, accuracy:0.978214, AUC:0.992821\n",
            "Epoch:623, d_loss:0.042399, g_loss:6.258398, accuracy:0.984083, AUC:0.998931\n",
            "Epoch:624, d_loss:0.052146, g_loss:7.073754, accuracy:0.987274, AUC:0.998216\n",
            "Epoch:625, d_loss:0.032688, g_loss:7.012028, accuracy:0.957310, AUC:0.990225\n",
            "Epoch:626, d_loss:0.056177, g_loss:6.189636, accuracy:0.955452, AUC:0.996435\n",
            "Epoch:627, d_loss:0.047241, g_loss:6.408651, accuracy:0.974345, AUC:0.993186\n",
            "Epoch:628, d_loss:0.047958, g_loss:6.680205, accuracy:0.965702, AUC:0.993850\n",
            "Epoch:629, d_loss:0.058129, g_loss:6.119968, accuracy:0.937845, AUC:0.991844\n",
            "Epoch:630, d_loss:0.043864, g_loss:6.541394, accuracy:0.943071, AUC:0.994402\n",
            "Epoch:631, d_loss:0.035397, g_loss:7.336193, accuracy:0.956440, AUC:0.998171\n",
            "Epoch:632, d_loss:0.035243, g_loss:6.359460, accuracy:0.963893, AUC:0.996614\n",
            "Epoch:633, d_loss:0.042147, g_loss:6.095382, accuracy:0.968500, AUC:0.993912\n",
            "Epoch:634, d_loss:0.052221, g_loss:7.141234, accuracy:0.965750, AUC:0.994883\n",
            "Epoch:635, d_loss:0.072727, g_loss:6.505577, accuracy:0.972679, AUC:0.997841\n",
            "Epoch:636, d_loss:0.041282, g_loss:6.192726, accuracy:0.905571, AUC:0.979989\n",
            "Epoch:637, d_loss:0.073814, g_loss:6.821803, accuracy:0.953274, AUC:0.991283\n",
            "Epoch:638, d_loss:0.058905, g_loss:6.069077, accuracy:0.964214, AUC:0.992420\n",
            "Epoch:639, d_loss:0.039652, g_loss:6.469642, accuracy:0.968512, AUC:0.995114\n",
            "Epoch:640, d_loss:0.032028, g_loss:6.689594, accuracy:0.948131, AUC:0.995183\n",
            "Epoch:641, d_loss:0.027893, g_loss:7.046008, accuracy:0.957940, AUC:0.996483\n",
            "Epoch:642, d_loss:0.031009, g_loss:6.414057, accuracy:0.968762, AUC:0.994103\n",
            "Epoch:643, d_loss:0.042155, g_loss:6.753715, accuracy:0.941012, AUC:0.994173\n",
            "Epoch:644, d_loss:0.063572, g_loss:6.138799, accuracy:0.957536, AUC:0.988105\n",
            "Epoch:645, d_loss:0.042423, g_loss:6.339612, accuracy:0.972321, AUC:0.998546\n",
            "Epoch:646, d_loss:0.048048, g_loss:5.832845, accuracy:0.968452, AUC:0.999107\n",
            "Epoch:647, d_loss:0.030468, g_loss:6.345960, accuracy:0.977952, AUC:0.995753\n",
            "Epoch:648, d_loss:0.031402, g_loss:7.145202, accuracy:0.950905, AUC:0.996968\n",
            "Epoch:649, d_loss:0.040476, g_loss:6.089065, accuracy:0.976929, AUC:0.995525\n",
            "Epoch:650, d_loss:0.047962, g_loss:6.812733, accuracy:0.947429, AUC:0.995761\n",
            "Epoch:651, d_loss:0.034226, g_loss:7.620169, accuracy:0.982214, AUC:0.996031\n",
            "Epoch:652, d_loss:0.044022, g_loss:6.979691, accuracy:0.962738, AUC:0.993231\n",
            "Epoch:653, d_loss:0.043795, g_loss:7.447377, accuracy:0.974881, AUC:0.998397\n",
            "Epoch:654, d_loss:0.039091, g_loss:6.434555, accuracy:0.986333, AUC:0.998306\n",
            "Epoch:655, d_loss:0.078730, g_loss:6.662035, accuracy:0.877381, AUC:0.990576\n",
            "Epoch:656, d_loss:0.037960, g_loss:6.611973, accuracy:0.952143, AUC:0.989674\n",
            "Epoch:657, d_loss:0.054780, g_loss:6.145339, accuracy:0.956560, AUC:0.987218\n",
            "Epoch:658, d_loss:0.068630, g_loss:6.517492, accuracy:0.907226, AUC:0.991247\n",
            "Epoch:659, d_loss:0.028163, g_loss:7.038533, accuracy:0.953762, AUC:0.989810\n",
            "Epoch:660, d_loss:0.053034, g_loss:6.329177, accuracy:0.950905, AUC:0.992941\n",
            "Epoch:661, d_loss:0.046058, g_loss:6.546999, accuracy:0.944631, AUC:0.989743\n",
            "Epoch:662, d_loss:0.052902, g_loss:6.734406, accuracy:0.960131, AUC:0.997547\n",
            "Epoch:663, d_loss:0.034664, g_loss:6.956970, accuracy:0.960655, AUC:0.997954\n",
            "Epoch:664, d_loss:0.037962, g_loss:6.893656, accuracy:0.976238, AUC:0.993886\n",
            "Epoch:665, d_loss:0.073270, g_loss:7.541569, accuracy:0.958083, AUC:0.995348\n",
            "Epoch:666, d_loss:0.038055, g_loss:7.827589, accuracy:0.978452, AUC:0.997114\n",
            "Epoch:667, d_loss:0.007263, g_loss:7.986039, accuracy:0.993964, AUC:0.999158\n",
            "Epoch:668, d_loss:0.002731, g_loss:10.009004, accuracy:0.998679, AUC:0.999979\n",
            "Epoch:669, d_loss:0.001494, g_loss:9.754778, accuracy:0.998833, AUC:0.999916\n",
            "Epoch:670, d_loss:0.008515, g_loss:8.578606, accuracy:0.984452, AUC:0.997880\n",
            "Epoch:671, d_loss:0.067328, g_loss:6.753811, accuracy:0.988095, AUC:0.996611\n",
            "Epoch:672, d_loss:0.051849, g_loss:10.521020, accuracy:0.971548, AUC:0.999823\n",
            "Epoch:673, d_loss:0.119570, g_loss:10.524007, accuracy:0.922190, AUC:0.994655\n",
            "Epoch:674, d_loss:0.055096, g_loss:6.233104, accuracy:0.962417, AUC:0.999217\n",
            "Epoch:675, d_loss:0.058685, g_loss:7.930224, accuracy:0.983595, AUC:0.998811\n",
            "Epoch:676, d_loss:0.071175, g_loss:7.152263, accuracy:0.926107, AUC:0.996205\n",
            "Epoch:677, d_loss:0.053297, g_loss:6.785782, accuracy:0.969357, AUC:0.992362\n",
            "Epoch:678, d_loss:0.025871, g_loss:7.346025, accuracy:0.969571, AUC:0.995236\n",
            "Epoch:679, d_loss:0.017458, g_loss:8.119982, accuracy:0.955095, AUC:0.993284\n",
            "Epoch:680, d_loss:0.038405, g_loss:6.463754, accuracy:0.966750, AUC:0.995973\n",
            "Epoch:681, d_loss:0.040763, g_loss:6.664981, accuracy:0.976440, AUC:0.998463\n",
            "Epoch:682, d_loss:0.045069, g_loss:6.942920, accuracy:0.949310, AUC:0.998775\n",
            "Epoch:683, d_loss:0.058901, g_loss:8.120893, accuracy:0.973571, AUC:0.997066\n",
            "Epoch:684, d_loss:0.038456, g_loss:6.570891, accuracy:0.956893, AUC:0.996334\n",
            "Epoch:685, d_loss:0.029958, g_loss:6.182960, accuracy:0.981298, AUC:0.995516\n",
            "Epoch:686, d_loss:0.030120, g_loss:6.951225, accuracy:0.908607, AUC:0.996101\n",
            "Epoch:687, d_loss:0.078775, g_loss:7.582980, accuracy:0.972452, AUC:0.993315\n",
            "Epoch:688, d_loss:0.051692, g_loss:8.005910, accuracy:0.993702, AUC:0.999097\n",
            "Epoch:689, d_loss:0.037032, g_loss:6.951746, accuracy:0.977857, AUC:0.994414\n",
            "Epoch:690, d_loss:0.024885, g_loss:7.198957, accuracy:0.980048, AUC:0.997525\n",
            "Epoch:691, d_loss:0.029955, g_loss:7.221480, accuracy:0.968143, AUC:0.998980\n",
            "Epoch:692, d_loss:0.033202, g_loss:6.306109, accuracy:0.981571, AUC:0.995666\n",
            "Epoch:693, d_loss:0.026532, g_loss:6.913582, accuracy:0.982571, AUC:0.996357\n",
            "Epoch:694, d_loss:0.048786, g_loss:6.342191, accuracy:0.961048, AUC:0.997731\n",
            "Epoch:695, d_loss:0.052119, g_loss:7.098371, accuracy:0.945810, AUC:0.993137\n",
            "Epoch:696, d_loss:0.024377, g_loss:8.023649, accuracy:0.991869, AUC:0.999004\n",
            "Epoch:697, d_loss:0.047187, g_loss:6.922573, accuracy:0.896488, AUC:0.996995\n",
            "Epoch:698, d_loss:0.086471, g_loss:7.595159, accuracy:0.974714, AUC:0.996532\n",
            "Epoch:699, d_loss:0.034229, g_loss:8.482066, accuracy:0.994952, AUC:0.999541\n",
            "Epoch:700, d_loss:0.028760, g_loss:6.664860, accuracy:0.982048, AUC:0.997912\n",
            "Epoch:701, d_loss:0.029914, g_loss:7.709159, accuracy:0.981488, AUC:0.996820\n",
            "Epoch:702, d_loss:0.044467, g_loss:6.781660, accuracy:0.991583, AUC:0.999227\n",
            "Epoch:703, d_loss:0.043494, g_loss:6.079352, accuracy:0.959964, AUC:0.993615\n",
            "Epoch:704, d_loss:0.040435, g_loss:6.864566, accuracy:0.886107, AUC:0.994660\n",
            "Epoch:705, d_loss:0.043601, g_loss:6.309301, accuracy:0.966238, AUC:0.998463\n",
            "Epoch:706, d_loss:0.031801, g_loss:6.345223, accuracy:0.992619, AUC:0.999605\n",
            "Epoch:707, d_loss:0.036235, g_loss:6.594301, accuracy:0.952464, AUC:0.996500\n",
            "Epoch:708, d_loss:0.041995, g_loss:6.401262, accuracy:0.954702, AUC:0.996182\n",
            "Epoch:709, d_loss:0.043766, g_loss:6.254369, accuracy:0.975345, AUC:0.994567\n",
            "Epoch:710, d_loss:0.069221, g_loss:6.804332, accuracy:0.907369, AUC:0.993833\n",
            "Epoch:711, d_loss:0.058976, g_loss:6.598988, accuracy:0.969179, AUC:0.997697\n",
            "Epoch:712, d_loss:0.048681, g_loss:6.893385, accuracy:0.964714, AUC:0.995205\n",
            "Epoch:713, d_loss:0.025557, g_loss:7.740595, accuracy:0.965643, AUC:0.998060\n",
            "Epoch:714, d_loss:0.026513, g_loss:7.096173, accuracy:0.978607, AUC:0.999087\n",
            "Epoch:715, d_loss:0.023718, g_loss:6.643444, accuracy:0.979952, AUC:0.997741\n",
            "Epoch:716, d_loss:0.046486, g_loss:6.018625, accuracy:0.950012, AUC:0.993444\n",
            "Epoch:717, d_loss:0.083714, g_loss:6.730209, accuracy:0.984452, AUC:0.996946\n",
            "Epoch:718, d_loss:0.034507, g_loss:7.249105, accuracy:0.971286, AUC:0.997188\n",
            "Epoch:719, d_loss:0.045776, g_loss:7.444814, accuracy:0.988548, AUC:0.998086\n",
            "Epoch:720, d_loss:0.045724, g_loss:5.995385, accuracy:0.936286, AUC:0.996049\n",
            "Epoch:721, d_loss:0.052081, g_loss:6.869102, accuracy:0.969107, AUC:0.994576\n",
            "Epoch:722, d_loss:0.057517, g_loss:6.309428, accuracy:0.949333, AUC:0.996636\n",
            "Epoch:723, d_loss:0.060551, g_loss:5.787531, accuracy:0.957107, AUC:0.996459\n",
            "Epoch:724, d_loss:0.056717, g_loss:7.017454, accuracy:0.938179, AUC:0.994072\n",
            "Epoch:725, d_loss:0.069241, g_loss:5.792590, accuracy:0.959643, AUC:0.992128\n",
            "Epoch:726, d_loss:0.041664, g_loss:6.905619, accuracy:0.869619, AUC:0.989637\n",
            "Epoch:727, d_loss:0.060153, g_loss:6.196055, accuracy:0.968964, AUC:0.993727\n",
            "Epoch:728, d_loss:0.044848, g_loss:6.444686, accuracy:0.965429, AUC:0.994126\n",
            "Epoch:729, d_loss:0.077505, g_loss:6.481470, accuracy:0.962333, AUC:0.992637\n",
            "Epoch:730, d_loss:0.044888, g_loss:6.144092, accuracy:0.944488, AUC:0.988290\n",
            "Epoch:731, d_loss:0.058486, g_loss:6.547708, accuracy:0.950512, AUC:0.996319\n",
            "Epoch:732, d_loss:0.059122, g_loss:6.453263, accuracy:0.976821, AUC:0.998239\n",
            "Epoch:733, d_loss:0.046465, g_loss:6.786506, accuracy:0.962095, AUC:0.993823\n",
            "Epoch:734, d_loss:0.049337, g_loss:6.039925, accuracy:0.930548, AUC:0.989246\n",
            "Epoch:735, d_loss:0.051481, g_loss:6.317745, accuracy:0.961655, AUC:0.995739\n",
            "Epoch:736, d_loss:0.066175, g_loss:6.545092, accuracy:0.946345, AUC:0.989752\n",
            "Epoch:737, d_loss:0.053030, g_loss:6.193751, accuracy:0.974155, AUC:0.995688\n",
            "Epoch:738, d_loss:0.062184, g_loss:6.716574, accuracy:0.963631, AUC:0.992477\n",
            "Epoch:739, d_loss:0.051011, g_loss:6.482669, accuracy:0.960810, AUC:0.990987\n",
            "Epoch:740, d_loss:0.054927, g_loss:6.072114, accuracy:0.969298, AUC:0.996588\n",
            "Epoch:741, d_loss:0.064950, g_loss:5.764610, accuracy:0.936679, AUC:0.985422\n",
            "Epoch:742, d_loss:0.082297, g_loss:6.398798, accuracy:0.970643, AUC:0.994749\n",
            "Epoch:743, d_loss:0.059602, g_loss:6.442389, accuracy:0.945250, AUC:0.991227\n",
            "Epoch:744, d_loss:0.046706, g_loss:6.135383, accuracy:0.923810, AUC:0.994217\n",
            "Epoch:745, d_loss:0.050732, g_loss:7.038027, accuracy:0.863405, AUC:0.992913\n",
            "Epoch:746, d_loss:0.082808, g_loss:6.803220, accuracy:0.961964, AUC:0.990408\n",
            "Epoch:747, d_loss:0.073886, g_loss:6.255124, accuracy:0.941762, AUC:0.991786\n",
            "Epoch:748, d_loss:0.058398, g_loss:6.326163, accuracy:0.945643, AUC:0.983374\n",
            "Epoch:749, d_loss:0.073898, g_loss:6.439614, accuracy:0.932845, AUC:0.996063\n",
            "Epoch:750, d_loss:0.046705, g_loss:6.525618, accuracy:0.906095, AUC:0.991504\n",
            "Epoch:751, d_loss:0.051868, g_loss:6.789697, accuracy:0.948202, AUC:0.990095\n",
            "Epoch:752, d_loss:0.039218, g_loss:6.385577, accuracy:0.944881, AUC:0.992575\n",
            "Epoch:753, d_loss:0.035340, g_loss:6.645638, accuracy:0.941440, AUC:0.991973\n",
            "Epoch:754, d_loss:0.070910, g_loss:6.201254, accuracy:0.925143, AUC:0.989815\n",
            "Epoch:755, d_loss:0.048696, g_loss:6.497420, accuracy:0.959321, AUC:0.991152\n",
            "Epoch:756, d_loss:0.059258, g_loss:6.225672, accuracy:0.931988, AUC:0.995787\n",
            "Epoch:757, d_loss:0.052513, g_loss:6.948988, accuracy:0.982940, AUC:0.997359\n",
            "Epoch:758, d_loss:0.035780, g_loss:6.115154, accuracy:0.937893, AUC:0.991329\n",
            "Epoch:759, d_loss:0.049518, g_loss:6.589560, accuracy:0.959940, AUC:0.996192\n",
            "Epoch:760, d_loss:0.058090, g_loss:6.342089, accuracy:0.953000, AUC:0.991182\n",
            "Epoch:761, d_loss:0.040472, g_loss:6.302588, accuracy:0.930131, AUC:0.989707\n",
            "Epoch:762, d_loss:0.058422, g_loss:6.667725, accuracy:0.941881, AUC:0.994155\n",
            "Epoch:763, d_loss:0.032039, g_loss:6.452065, accuracy:0.913060, AUC:0.993584\n",
            "Epoch:764, d_loss:0.030632, g_loss:7.088798, accuracy:0.979298, AUC:0.998120\n",
            "Epoch:765, d_loss:0.034284, g_loss:6.696490, accuracy:0.970024, AUC:0.998984\n",
            "Epoch:766, d_loss:0.041102, g_loss:6.190868, accuracy:0.967190, AUC:0.993645\n",
            "Epoch:767, d_loss:0.079979, g_loss:6.435968, accuracy:0.949000, AUC:0.993831\n",
            "Epoch:768, d_loss:0.063039, g_loss:6.584158, accuracy:0.962143, AUC:0.988893\n",
            "Epoch:769, d_loss:0.060462, g_loss:6.217797, accuracy:0.974583, AUC:0.994322\n",
            "Epoch:770, d_loss:0.046313, g_loss:6.745296, accuracy:0.978905, AUC:0.995571\n",
            "Epoch:771, d_loss:0.055232, g_loss:6.802679, accuracy:0.903167, AUC:0.997769\n",
            "Epoch:772, d_loss:0.058066, g_loss:6.747044, accuracy:0.969702, AUC:0.995191\n",
            "Epoch:773, d_loss:0.046471, g_loss:6.124835, accuracy:0.956524, AUC:0.992990\n",
            "Epoch:774, d_loss:0.062976, g_loss:6.366975, accuracy:0.922000, AUC:0.995975\n",
            "Epoch:775, d_loss:0.050222, g_loss:5.930502, accuracy:0.930024, AUC:0.987021\n",
            "Epoch:776, d_loss:0.050428, g_loss:6.129727, accuracy:0.867179, AUC:0.994691\n",
            "Epoch:777, d_loss:0.083320, g_loss:6.388453, accuracy:0.882202, AUC:0.969135\n",
            "Epoch:778, d_loss:0.049214, g_loss:6.350709, accuracy:0.969298, AUC:0.991231\n",
            "Epoch:779, d_loss:0.052847, g_loss:7.323227, accuracy:0.979452, AUC:0.996000\n",
            "Epoch:780, d_loss:0.043588, g_loss:7.259861, accuracy:0.967286, AUC:0.991733\n",
            "Epoch:781, d_loss:0.022664, g_loss:7.889815, accuracy:0.985845, AUC:0.999113\n",
            "Epoch:782, d_loss:0.016305, g_loss:6.760779, accuracy:0.945952, AUC:0.995626\n",
            "Epoch:783, d_loss:0.035437, g_loss:6.651965, accuracy:0.971476, AUC:0.994227\n",
            "Epoch:784, d_loss:0.034994, g_loss:6.345321, accuracy:0.967048, AUC:0.994901\n",
            "Epoch:785, d_loss:0.065218, g_loss:6.519151, accuracy:0.975226, AUC:0.996627\n",
            "Epoch:786, d_loss:0.040717, g_loss:5.979497, accuracy:0.954595, AUC:0.993076\n",
            "Epoch:787, d_loss:0.046902, g_loss:6.581699, accuracy:0.962119, AUC:0.997968\n",
            "Epoch:788, d_loss:0.036461, g_loss:6.299489, accuracy:0.970964, AUC:0.993228\n",
            "Epoch:789, d_loss:0.047052, g_loss:6.626151, accuracy:0.980524, AUC:0.997456\n",
            "Epoch:790, d_loss:0.054147, g_loss:6.189264, accuracy:0.979952, AUC:0.998608\n",
            "Epoch:791, d_loss:0.036652, g_loss:6.371832, accuracy:0.956679, AUC:0.996037\n",
            "Epoch:792, d_loss:0.049623, g_loss:6.185697, accuracy:0.969607, AUC:0.993463\n",
            "Epoch:793, d_loss:0.058603, g_loss:6.521834, accuracy:0.972226, AUC:0.993151\n",
            "Epoch:794, d_loss:0.048088, g_loss:6.465428, accuracy:0.968536, AUC:0.996852\n",
            "Epoch:795, d_loss:0.064483, g_loss:6.037028, accuracy:0.958643, AUC:0.989798\n",
            "Epoch:796, d_loss:0.039701, g_loss:6.325105, accuracy:0.968571, AUC:0.994013\n",
            "Epoch:797, d_loss:0.047883, g_loss:6.503516, accuracy:0.940369, AUC:0.991283\n",
            "Epoch:798, d_loss:0.054464, g_loss:6.088768, accuracy:0.983310, AUC:0.997982\n",
            "Epoch:799, d_loss:0.052738, g_loss:6.451653, accuracy:0.913929, AUC:0.993116\n",
            "Epoch:800, d_loss:0.061570, g_loss:6.047213, accuracy:0.957667, AUC:0.991388\n",
            "Epoch:801, d_loss:0.049308, g_loss:6.642086, accuracy:0.924702, AUC:0.984581\n",
            "Epoch:802, d_loss:0.070149, g_loss:6.639085, accuracy:0.982119, AUC:0.997960\n",
            "Epoch:803, d_loss:0.062472, g_loss:6.043702, accuracy:0.895690, AUC:0.990326\n",
            "Epoch:804, d_loss:0.038254, g_loss:6.447626, accuracy:0.942714, AUC:0.993954\n",
            "Epoch:805, d_loss:0.074524, g_loss:5.899960, accuracy:0.975548, AUC:0.997730\n",
            "Epoch:806, d_loss:0.047628, g_loss:6.487861, accuracy:0.947476, AUC:0.988710\n",
            "Epoch:807, d_loss:0.040210, g_loss:6.169246, accuracy:0.959619, AUC:0.994977\n",
            "Epoch:808, d_loss:0.039376, g_loss:6.775646, accuracy:0.955714, AUC:0.998443\n",
            "Epoch:809, d_loss:0.041920, g_loss:6.199607, accuracy:0.988786, AUC:0.999146\n",
            "Epoch:810, d_loss:0.033152, g_loss:6.685399, accuracy:0.965500, AUC:0.995048\n",
            "Epoch:811, d_loss:0.049416, g_loss:7.182123, accuracy:0.980381, AUC:0.995359\n",
            "Epoch:812, d_loss:0.040517, g_loss:7.490278, accuracy:0.976298, AUC:0.992839\n",
            "Epoch:813, d_loss:0.035455, g_loss:6.352841, accuracy:0.955179, AUC:0.994212\n",
            "Epoch:814, d_loss:0.026803, g_loss:6.917492, accuracy:0.943774, AUC:0.993985\n",
            "Epoch:815, d_loss:0.033495, g_loss:6.608064, accuracy:0.982119, AUC:0.998599\n",
            "Epoch:816, d_loss:0.046943, g_loss:6.442892, accuracy:0.969345, AUC:0.998163\n",
            "Epoch:817, d_loss:0.049265, g_loss:5.978996, accuracy:0.981167, AUC:0.998613\n",
            "Epoch:818, d_loss:0.060406, g_loss:6.240485, accuracy:0.954726, AUC:0.997242\n",
            "Epoch:819, d_loss:0.071366, g_loss:6.279162, accuracy:0.968179, AUC:0.995162\n",
            "Epoch:820, d_loss:0.060811, g_loss:6.417809, accuracy:0.940214, AUC:0.997108\n",
            "Epoch:821, d_loss:0.058575, g_loss:6.492279, accuracy:0.952155, AUC:0.991807\n",
            "Epoch:822, d_loss:0.036396, g_loss:6.646622, accuracy:0.979071, AUC:0.996368\n",
            "Epoch:823, d_loss:0.040377, g_loss:6.655688, accuracy:0.919548, AUC:0.996866\n",
            "Epoch:824, d_loss:0.065438, g_loss:6.314425, accuracy:0.980262, AUC:0.996674\n",
            "Epoch:825, d_loss:0.067083, g_loss:6.423143, accuracy:0.963333, AUC:0.993553\n",
            "Epoch:826, d_loss:0.075293, g_loss:5.910314, accuracy:0.795988, AUC:0.968979\n",
            "Epoch:827, d_loss:0.056055, g_loss:6.267510, accuracy:0.952774, AUC:0.991509\n",
            "Epoch:828, d_loss:0.083844, g_loss:5.763271, accuracy:0.907095, AUC:0.995274\n",
            "Epoch:829, d_loss:0.039974, g_loss:6.390296, accuracy:0.975595, AUC:0.994821\n",
            "Epoch:830, d_loss:0.050067, g_loss:6.369016, accuracy:0.943929, AUC:0.994507\n",
            "Epoch:831, d_loss:0.041250, g_loss:6.393596, accuracy:0.966048, AUC:0.996203\n",
            "Epoch:832, d_loss:0.045326, g_loss:6.374481, accuracy:0.964488, AUC:0.995692\n",
            "Epoch:833, d_loss:0.035734, g_loss:6.547017, accuracy:0.971869, AUC:0.996087\n",
            "Epoch:834, d_loss:0.045200, g_loss:6.085050, accuracy:0.965881, AUC:0.993533\n",
            "Epoch:835, d_loss:0.044013, g_loss:6.134573, accuracy:0.958631, AUC:0.995209\n",
            "Epoch:836, d_loss:0.031003, g_loss:7.078131, accuracy:0.979607, AUC:0.998570\n",
            "Epoch:837, d_loss:0.038256, g_loss:6.233375, accuracy:0.901417, AUC:0.991497\n",
            "Epoch:838, d_loss:0.050208, g_loss:6.855663, accuracy:0.983107, AUC:0.997731\n",
            "Epoch:839, d_loss:0.044146, g_loss:6.359054, accuracy:0.924012, AUC:0.978863\n",
            "Epoch:840, d_loss:0.049725, g_loss:6.286205, accuracy:0.939786, AUC:0.998695\n",
            "Epoch:841, d_loss:0.045908, g_loss:5.674884, accuracy:0.946274, AUC:0.994650\n",
            "Epoch:842, d_loss:0.051509, g_loss:6.727169, accuracy:0.979679, AUC:0.999264\n",
            "Epoch:843, d_loss:0.057500, g_loss:6.571816, accuracy:0.963464, AUC:0.990433\n",
            "Epoch:844, d_loss:0.050107, g_loss:6.202355, accuracy:0.919417, AUC:0.994599\n",
            "Epoch:845, d_loss:0.063500, g_loss:6.641345, accuracy:0.945083, AUC:0.982978\n",
            "Epoch:846, d_loss:0.048366, g_loss:6.638745, accuracy:0.934488, AUC:0.990887\n",
            "Epoch:847, d_loss:0.055924, g_loss:6.401464, accuracy:0.981595, AUC:0.997113\n",
            "Epoch:848, d_loss:0.065718, g_loss:6.589753, accuracy:0.987917, AUC:0.999357\n",
            "Epoch:849, d_loss:0.030989, g_loss:6.660202, accuracy:0.943393, AUC:0.992067\n",
            "Epoch:850, d_loss:0.039043, g_loss:6.640001, accuracy:0.972250, AUC:0.994679\n",
            "Epoch:851, d_loss:0.055287, g_loss:6.249572, accuracy:0.985762, AUC:0.999233\n",
            "Epoch:852, d_loss:0.042342, g_loss:6.075469, accuracy:0.938893, AUC:0.994163\n",
            "Epoch:853, d_loss:0.051569, g_loss:6.088768, accuracy:0.950381, AUC:0.993542\n",
            "Epoch:854, d_loss:0.049519, g_loss:6.509756, accuracy:0.913107, AUC:0.995538\n",
            "Epoch:855, d_loss:0.057108, g_loss:6.475129, accuracy:0.900405, AUC:0.991681\n",
            "Epoch:856, d_loss:0.074640, g_loss:5.737512, accuracy:0.953381, AUC:0.992222\n",
            "Epoch:857, d_loss:0.053732, g_loss:6.513522, accuracy:0.978083, AUC:0.997369\n",
            "Epoch:858, d_loss:0.065069, g_loss:5.875913, accuracy:0.933571, AUC:0.988107\n",
            "Epoch:859, d_loss:0.045825, g_loss:6.217003, accuracy:0.964893, AUC:0.993620\n",
            "Epoch:860, d_loss:0.051732, g_loss:6.270967, accuracy:0.926583, AUC:0.991834\n",
            "Epoch:861, d_loss:0.051351, g_loss:6.488539, accuracy:0.883250, AUC:0.992300\n",
            "Epoch:862, d_loss:0.086967, g_loss:6.257704, accuracy:0.925667, AUC:0.986157\n",
            "Epoch:863, d_loss:0.072707, g_loss:6.242840, accuracy:0.959488, AUC:0.990481\n",
            "Epoch:864, d_loss:0.050424, g_loss:5.914678, accuracy:0.902536, AUC:0.986817\n",
            "Epoch:865, d_loss:0.044559, g_loss:6.603452, accuracy:0.986238, AUC:0.998446\n",
            "Epoch:866, d_loss:0.042965, g_loss:6.205927, accuracy:0.965810, AUC:0.995192\n",
            "Epoch:867, d_loss:0.045909, g_loss:6.139500, accuracy:0.969786, AUC:0.996879\n",
            "Epoch:868, d_loss:0.054775, g_loss:5.934435, accuracy:0.947190, AUC:0.988290\n",
            "Epoch:869, d_loss:0.048612, g_loss:6.472401, accuracy:0.942190, AUC:0.988263\n",
            "Epoch:870, d_loss:0.064561, g_loss:6.437124, accuracy:0.956619, AUC:0.991918\n",
            "Epoch:871, d_loss:0.057143, g_loss:6.265201, accuracy:0.935726, AUC:0.989549\n",
            "Epoch:872, d_loss:0.057835, g_loss:6.095241, accuracy:0.966333, AUC:0.994147\n",
            "Epoch:873, d_loss:0.043825, g_loss:6.747066, accuracy:0.950024, AUC:0.995708\n",
            "Epoch:874, d_loss:0.062667, g_loss:6.290565, accuracy:0.905940, AUC:0.988440\n",
            "Epoch:875, d_loss:0.048810, g_loss:6.812068, accuracy:0.985619, AUC:0.998214\n",
            "Epoch:876, d_loss:0.042522, g_loss:6.916978, accuracy:0.989714, AUC:0.998842\n",
            "Epoch:877, d_loss:0.051797, g_loss:6.003601, accuracy:0.956310, AUC:0.994656\n",
            "Epoch:878, d_loss:0.055881, g_loss:6.315228, accuracy:0.892536, AUC:0.992178\n",
            "Epoch:879, d_loss:0.055274, g_loss:6.143011, accuracy:0.970643, AUC:0.996704\n",
            "Epoch:880, d_loss:0.044875, g_loss:6.401797, accuracy:0.942714, AUC:0.993802\n",
            "Epoch:881, d_loss:0.089062, g_loss:6.500970, accuracy:0.984286, AUC:0.998040\n",
            "Epoch:882, d_loss:0.079924, g_loss:6.338930, accuracy:0.969786, AUC:0.994724\n",
            "Epoch:883, d_loss:0.041167, g_loss:6.943851, accuracy:0.955226, AUC:0.988828\n",
            "Epoch:884, d_loss:0.036643, g_loss:6.548717, accuracy:0.957310, AUC:0.996552\n",
            "Epoch:885, d_loss:0.057075, g_loss:6.395934, accuracy:0.934476, AUC:0.997833\n",
            "Epoch:886, d_loss:0.067701, g_loss:7.070584, accuracy:0.979536, AUC:0.998295\n",
            "Epoch:887, d_loss:0.049815, g_loss:6.213303, accuracy:0.937357, AUC:0.989452\n",
            "Epoch:888, d_loss:0.069570, g_loss:6.588978, accuracy:0.946357, AUC:0.995588\n",
            "Epoch:889, d_loss:0.055905, g_loss:6.244184, accuracy:0.950905, AUC:0.992415\n",
            "Epoch:890, d_loss:0.043125, g_loss:5.688738, accuracy:0.946238, AUC:0.993205\n",
            "Epoch:891, d_loss:0.055375, g_loss:5.781553, accuracy:0.955226, AUC:0.993745\n",
            "Epoch:892, d_loss:0.065406, g_loss:5.964412, accuracy:0.916607, AUC:0.991464\n",
            "Epoch:893, d_loss:0.060808, g_loss:6.162700, accuracy:0.923345, AUC:0.991817\n",
            "Epoch:894, d_loss:0.096815, g_loss:6.001199, accuracy:0.954524, AUC:0.997648\n",
            "Epoch:895, d_loss:0.051465, g_loss:6.147784, accuracy:0.888571, AUC:0.986685\n",
            "Epoch:896, d_loss:0.074187, g_loss:6.412102, accuracy:0.942000, AUC:0.990693\n",
            "Epoch:897, d_loss:0.059741, g_loss:5.977873, accuracy:0.955024, AUC:0.990923\n",
            "Epoch:898, d_loss:0.050437, g_loss:6.464485, accuracy:0.940333, AUC:0.982541\n",
            "Epoch:899, d_loss:0.052633, g_loss:7.091562, accuracy:0.972905, AUC:0.998434\n",
            "Epoch:900, d_loss:0.027997, g_loss:7.249785, accuracy:0.900369, AUC:0.995224\n",
            "Epoch:901, d_loss:0.048155, g_loss:6.185154, accuracy:0.979821, AUC:0.997285\n",
            "Epoch:902, d_loss:0.056045, g_loss:6.114388, accuracy:0.945143, AUC:0.996084\n",
            "Epoch:903, d_loss:0.056273, g_loss:5.864054, accuracy:0.948238, AUC:0.998921\n",
            "Epoch:904, d_loss:0.057049, g_loss:5.832491, accuracy:0.909405, AUC:0.983878\n",
            "Epoch:905, d_loss:0.058681, g_loss:5.928874, accuracy:0.852226, AUC:0.983543\n",
            "Epoch:906, d_loss:0.087825, g_loss:6.007304, accuracy:0.971488, AUC:0.996653\n",
            "Epoch:907, d_loss:0.064667, g_loss:6.207166, accuracy:0.915952, AUC:0.989267\n",
            "Epoch:908, d_loss:0.059221, g_loss:5.980638, accuracy:0.871762, AUC:0.971432\n",
            "Epoch:909, d_loss:0.046570, g_loss:6.486034, accuracy:0.968429, AUC:0.994311\n",
            "Epoch:910, d_loss:0.055775, g_loss:6.351375, accuracy:0.934333, AUC:0.985493\n",
            "Epoch:911, d_loss:0.036802, g_loss:6.744352, accuracy:0.970655, AUC:0.993226\n",
            "Epoch:912, d_loss:0.079048, g_loss:6.124362, accuracy:0.922107, AUC:0.987249\n",
            "Epoch:913, d_loss:0.076575, g_loss:5.976416, accuracy:0.928643, AUC:0.986267\n",
            "Epoch:914, d_loss:0.053055, g_loss:6.183262, accuracy:0.881786, AUC:0.983308\n",
            "Epoch:915, d_loss:0.058174, g_loss:6.032099, accuracy:0.933393, AUC:0.996097\n",
            "Epoch:916, d_loss:0.053008, g_loss:6.209484, accuracy:0.958762, AUC:0.991387\n",
            "Epoch:917, d_loss:0.055938, g_loss:6.247610, accuracy:0.973821, AUC:0.996275\n",
            "Epoch:918, d_loss:0.046016, g_loss:6.296169, accuracy:0.942012, AUC:0.987289\n",
            "Epoch:919, d_loss:0.060628, g_loss:6.056968, accuracy:0.951905, AUC:0.993975\n",
            "Epoch:920, d_loss:0.060542, g_loss:6.179274, accuracy:0.956571, AUC:0.996839\n",
            "Epoch:921, d_loss:0.053379, g_loss:6.047444, accuracy:0.968262, AUC:0.991072\n",
            "Epoch:922, d_loss:0.044616, g_loss:5.536117, accuracy:0.946202, AUC:0.991550\n",
            "Epoch:923, d_loss:0.044418, g_loss:6.216148, accuracy:0.975833, AUC:0.996722\n",
            "Epoch:924, d_loss:0.056938, g_loss:6.081859, accuracy:0.971369, AUC:0.995234\n",
            "Epoch:925, d_loss:0.053765, g_loss:6.099254, accuracy:0.961631, AUC:0.991413\n",
            "Epoch:926, d_loss:0.057789, g_loss:5.622567, accuracy:0.900060, AUC:0.993080\n",
            "Epoch:927, d_loss:0.077601, g_loss:6.504409, accuracy:0.955369, AUC:0.994302\n",
            "Epoch:928, d_loss:0.059792, g_loss:6.350208, accuracy:0.961786, AUC:0.988632\n",
            "Epoch:929, d_loss:0.059640, g_loss:6.271517, accuracy:0.968321, AUC:0.995301\n",
            "Epoch:930, d_loss:0.062537, g_loss:5.512569, accuracy:0.919119, AUC:0.985402\n",
            "Epoch:931, d_loss:0.036456, g_loss:6.731807, accuracy:0.943952, AUC:0.995456\n",
            "Epoch:932, d_loss:0.047118, g_loss:5.937841, accuracy:0.936214, AUC:0.993504\n",
            "Epoch:933, d_loss:0.040962, g_loss:6.705136, accuracy:0.937226, AUC:0.992905\n",
            "Epoch:934, d_loss:0.042148, g_loss:6.306165, accuracy:0.969607, AUC:0.992512\n",
            "Epoch:935, d_loss:0.059829, g_loss:6.555256, accuracy:0.975810, AUC:0.994841\n",
            "Epoch:936, d_loss:0.060496, g_loss:5.951395, accuracy:0.966286, AUC:0.990893\n",
            "Epoch:937, d_loss:0.048998, g_loss:6.454947, accuracy:0.978583, AUC:0.996951\n",
            "Epoch:938, d_loss:0.073829, g_loss:5.765299, accuracy:0.956155, AUC:0.993966\n",
            "Epoch:939, d_loss:0.057241, g_loss:6.185933, accuracy:0.899762, AUC:0.991832\n",
            "Epoch:940, d_loss:0.061648, g_loss:5.952667, accuracy:0.967464, AUC:0.991839\n",
            "Epoch:941, d_loss:0.040536, g_loss:6.555082, accuracy:0.930976, AUC:0.991872\n",
            "Epoch:942, d_loss:0.037689, g_loss:6.415220, accuracy:0.978560, AUC:0.998321\n",
            "Epoch:943, d_loss:0.068327, g_loss:6.305460, accuracy:0.908976, AUC:0.987475\n",
            "Epoch:944, d_loss:0.061089, g_loss:5.527789, accuracy:0.944667, AUC:0.985549\n",
            "Epoch:945, d_loss:0.047724, g_loss:6.283399, accuracy:0.970238, AUC:0.995152\n",
            "Epoch:946, d_loss:0.084837, g_loss:6.050850, accuracy:0.881881, AUC:0.996573\n",
            "Epoch:947, d_loss:0.045635, g_loss:6.171925, accuracy:0.903012, AUC:0.993824\n",
            "Epoch:948, d_loss:0.067837, g_loss:6.361597, accuracy:0.931179, AUC:0.984865\n",
            "Epoch:949, d_loss:0.051785, g_loss:6.625034, accuracy:0.982964, AUC:0.996294\n",
            "Epoch:950, d_loss:0.054166, g_loss:6.507743, accuracy:0.968774, AUC:0.993338\n",
            "Epoch:951, d_loss:0.047979, g_loss:7.180946, accuracy:0.967548, AUC:0.994078\n",
            "Epoch:952, d_loss:0.075311, g_loss:6.864224, accuracy:0.967369, AUC:0.992267\n",
            "Epoch:953, d_loss:0.059323, g_loss:6.020313, accuracy:0.919583, AUC:0.975042\n",
            "Epoch:954, d_loss:0.052121, g_loss:5.843670, accuracy:0.933583, AUC:0.992046\n",
            "Epoch:955, d_loss:0.049963, g_loss:6.175021, accuracy:0.948595, AUC:0.996748\n",
            "Epoch:956, d_loss:0.043914, g_loss:6.024992, accuracy:0.919500, AUC:0.975970\n",
            "Epoch:957, d_loss:0.068878, g_loss:6.017804, accuracy:0.948131, AUC:0.995088\n",
            "Epoch:958, d_loss:0.048329, g_loss:6.680690, accuracy:0.943190, AUC:0.987684\n",
            "Epoch:959, d_loss:0.046004, g_loss:6.683577, accuracy:0.967143, AUC:0.992467\n",
            "Epoch:960, d_loss:0.071022, g_loss:6.527287, accuracy:0.924536, AUC:0.995719\n",
            "Epoch:961, d_loss:0.050416, g_loss:6.113553, accuracy:0.944381, AUC:0.985742\n",
            "Epoch:962, d_loss:0.061513, g_loss:6.188927, accuracy:0.928310, AUC:0.994842\n",
            "Epoch:963, d_loss:0.051592, g_loss:5.422266, accuracy:0.938095, AUC:0.983428\n",
            "Epoch:964, d_loss:0.053419, g_loss:6.266979, accuracy:0.982488, AUC:0.997485\n",
            "Epoch:965, d_loss:0.068747, g_loss:5.745604, accuracy:0.980071, AUC:0.997242\n",
            "Epoch:966, d_loss:0.056099, g_loss:5.816677, accuracy:0.929619, AUC:0.988582\n",
            "Epoch:967, d_loss:0.069029, g_loss:5.606614, accuracy:0.938929, AUC:0.984468\n",
            "Epoch:968, d_loss:0.052330, g_loss:6.685554, accuracy:0.966286, AUC:0.992857\n",
            "Epoch:969, d_loss:0.078421, g_loss:5.937896, accuracy:0.960190, AUC:0.993332\n",
            "Epoch:970, d_loss:0.068676, g_loss:5.820796, accuracy:0.926833, AUC:0.986116\n",
            "Epoch:971, d_loss:0.047862, g_loss:6.597004, accuracy:0.926964, AUC:0.992545\n",
            "Epoch:972, d_loss:0.047630, g_loss:6.053595, accuracy:0.899083, AUC:0.991021\n",
            "Epoch:973, d_loss:0.051511, g_loss:6.421513, accuracy:0.966333, AUC:0.995349\n",
            "Epoch:974, d_loss:0.096235, g_loss:6.048873, accuracy:0.903833, AUC:0.994957\n",
            "Epoch:975, d_loss:0.056536, g_loss:6.279957, accuracy:0.963357, AUC:0.990454\n",
            "Epoch:976, d_loss:0.077608, g_loss:6.335666, accuracy:0.982988, AUC:0.996750\n",
            "Epoch:977, d_loss:0.075633, g_loss:6.107123, accuracy:0.898071, AUC:0.989368\n",
            "Epoch:978, d_loss:0.034296, g_loss:6.341204, accuracy:0.949476, AUC:0.992026\n",
            "Epoch:979, d_loss:0.047852, g_loss:6.166316, accuracy:0.966226, AUC:0.996969\n",
            "Epoch:980, d_loss:0.054634, g_loss:5.592429, accuracy:0.938226, AUC:0.992395\n",
            "Epoch:981, d_loss:0.040857, g_loss:6.116106, accuracy:0.953012, AUC:0.996183\n",
            "Epoch:982, d_loss:0.055925, g_loss:5.761130, accuracy:0.967786, AUC:0.993501\n",
            "Epoch:983, d_loss:0.055739, g_loss:6.033207, accuracy:0.950952, AUC:0.988779\n",
            "Epoch:984, d_loss:0.051704, g_loss:6.005128, accuracy:0.951595, AUC:0.995207\n",
            "Epoch:985, d_loss:0.042929, g_loss:5.979779, accuracy:0.912298, AUC:0.990399\n",
            "Epoch:986, d_loss:0.052661, g_loss:6.202532, accuracy:0.964833, AUC:0.993287\n",
            "Epoch:987, d_loss:0.063774, g_loss:5.967766, accuracy:0.953167, AUC:0.998091\n",
            "Epoch:988, d_loss:0.047411, g_loss:6.017079, accuracy:0.943202, AUC:0.993992\n",
            "Epoch:989, d_loss:0.047227, g_loss:6.456675, accuracy:0.891988, AUC:0.989876\n",
            "Epoch:990, d_loss:0.049697, g_loss:6.598006, accuracy:0.886774, AUC:0.982403\n",
            "Epoch:991, d_loss:0.064780, g_loss:5.881199, accuracy:0.946893, AUC:0.992735\n",
            "Epoch:992, d_loss:0.079595, g_loss:6.216862, accuracy:0.956083, AUC:0.992434\n",
            "Epoch:993, d_loss:0.069618, g_loss:6.043384, accuracy:0.948774, AUC:0.986562\n",
            "Epoch:994, d_loss:0.052981, g_loss:6.035883, accuracy:0.958000, AUC:0.993059\n",
            "Epoch:995, d_loss:0.047639, g_loss:6.942408, accuracy:0.958762, AUC:0.993498\n",
            "Epoch:996, d_loss:0.050176, g_loss:6.453230, accuracy:0.947143, AUC:0.995402\n",
            "Epoch:997, d_loss:0.037245, g_loss:5.907924, accuracy:0.938131, AUC:0.991861\n",
            "Epoch:998, d_loss:0.049532, g_loss:6.055163, accuracy:0.947679, AUC:0.991139\n",
            "Epoch:999, d_loss:0.091373, g_loss:6.281450, accuracy:0.947131, AUC:0.991033\n",
            "/content/gdrive/My Drive/GOSH/Synthetic Data/medgan/binary/models/-999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IR9fN4KB3oX_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}